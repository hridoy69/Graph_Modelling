{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb16e8ff-d21a-4873-8dd5-b42f35432bf4",
   "metadata": {},
   "source": [
    "# Model Development\n",
    "- Train/Test Split > Tuning > Cross Verification > Training > Testing > Model Saved\n",
    "- Traing Loss and Training AUC Plot and Test ROC plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47d7def-7334-46ba-8f92-487a96d723aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined_transformer_gcn_pipeline_optuna_cv.py\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "from torch_geometric.data import DataLoader, Data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import rdMolDescriptors\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, accuracy_score, matthews_corrcoef,\n",
    "    cohen_kappa_score, brier_score_loss, confusion_matrix,\n",
    "    precision_score, f1_score, roc_curve\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "\n",
    "# Ensure output directory\n",
    "output_dir = 'Transformer_GCN'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Expanded atom features\n",
    "def atom_features(atom):\n",
    "    return torch.tensor([\n",
    "        # Basic properties\n",
    "        atom.GetAtomicNum(),                     # Atomic number\n",
    "        atom.GetDegree(),                        # Number of bonded neighbors\n",
    "        atom.GetFormalCharge(),                  # Formal charge\n",
    "        atom.GetNumRadicalElectrons(),           # Number of radical electrons\n",
    "        int(atom.GetIsAromatic()),               # Aromaticity flag\n",
    "        \n",
    "        # Extended properties\n",
    "        atom.GetExplicitValence(),               # Explicit valence\n",
    "        atom.GetImplicitValence(),               # Implicit valence\n",
    "        atom.GetTotalValence(),                  # Total valence\n",
    "        atom.GetNumImplicitHs(),                 # Number of implicit hydrogens\n",
    "        atom.GetHybridization(),                 # Hybridization state\n",
    "        atom.GetTotalNumHs(),                    # Total number of hydrogens\n",
    "        \n",
    "        # Topological properties\n",
    "        int(atom.IsInRing()),                    # Whether the atom is in a ring\n",
    "        int(atom.IsInRingSize(3)),               # Whether in 3-membered ring\n",
    "        int(atom.IsInRingSize(4)),               # Whether in 4-membered ring\n",
    "        int(atom.IsInRingSize(5)),               # Whether in 5-membered ring\n",
    "        int(atom.IsInRingSize(6)),               # Whether in 6-membered ring\n",
    "        int(atom.IsInRingSize(7)),               # Whether in 7-membered ring\n",
    "        \n",
    "        # Electronic properties\n",
    "        atom.GetChiralTag(),                     # Chirality\n",
    "        atom.GetMass(),                          # Atomic mass\n",
    "        Chem.rdMolDescriptors.CalcCrippenDescriptors(\n",
    "            Chem.MolFromSmiles(f\"[{atom.GetSymbol()}]\"))[0]  # LogP contribution\n",
    "    ], dtype=torch.float)\n",
    "\n",
    "# Molecule to PyTorch Geometric graph\n",
    "def mol_to_graph(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None: return None\n",
    "    atoms = [atom_features(atom) for atom in mol.GetAtoms()]\n",
    "    if not atoms: return None\n",
    "    x = torch.stack(atoms, dim=0)\n",
    "    edge_index = []\n",
    "    for bond in mol.GetBonds():\n",
    "        i, j = bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()\n",
    "        edge_index += [[i, j], [j, i]]\n",
    "    edge_index = torch.tensor(edge_index).t().contiguous()\n",
    "    edge_attr = [[bond.GetBondTypeAsDouble(), bond.GetIsConjugated(), bond.IsInRing()] * 2 for bond in mol.GetBonds()]\n",
    "    edge_attr = torch.tensor(edge_attr, dtype=torch.float).reshape(-1, 3)\n",
    "    return Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "\n",
    "# Load dataset\n",
    "def load_dataset(path, smiles_col='Smiles', target_col='Target'):\n",
    "    df = pd.read_excel(path)\n",
    "    graphs = []\n",
    "    for _, row in df.iterrows():\n",
    "        if pd.isna(row[smiles_col]) or pd.isna(row[target_col]):\n",
    "            continue\n",
    "        g = mol_to_graph(str(row[smiles_col]))\n",
    "        if g is not None:\n",
    "            g.y = torch.tensor([row[target_col]], dtype=torch.float)\n",
    "            g.smiles = row[smiles_col]\n",
    "            graphs.append(g)\n",
    "    return graphs\n",
    "\n",
    "# MultiHeadAttention Module\n",
    "class MultiHeadAttention(torch.nn.Module):\n",
    "    def __init__(self, hidden_dim, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        # Ensure hidden_dim is divisible by num_heads\n",
    "        # If not, we'll adjust the head_dim to make it work\n",
    "        self.head_dim = hidden_dim // num_heads\n",
    "        \n",
    "        # Actual dimension after ensuring divisibility\n",
    "        self.actual_dim = self.head_dim * num_heads\n",
    "        \n",
    "        self.query = torch.nn.Linear(hidden_dim, self.actual_dim)\n",
    "        self.key = torch.nn.Linear(hidden_dim, self.actual_dim)\n",
    "        self.value = torch.nn.Linear(hidden_dim, self.actual_dim)\n",
    "        self.proj = torch.nn.Linear(self.actual_dim, hidden_dim)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        seq_length = x.size(1)\n",
    "        \n",
    "        # Linear projections\n",
    "        q = self.query(x).view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k = self.key(x).view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        v = self.value(x).view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # Scaled dot-product attention\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "        \n",
    "        # Apply attention weights\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        context = torch.matmul(attn_weights, v)\n",
    "        \n",
    "        # Concatenate heads and put through final linear layer\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, seq_length, self.actual_dim)\n",
    "        output = self.proj(context)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Transformer Block\n",
    "class TransformerBlock(torch.nn.Module):\n",
    "    def __init__(self, hidden_dim, num_heads, ff_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(hidden_dim, num_heads, dropout)\n",
    "        self.norm1 = torch.nn.LayerNorm(hidden_dim)\n",
    "        self.norm2 = torch.nn.LayerNorm(hidden_dim)\n",
    "        self.ff = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_dim, ff_dim),\n",
    "            torch.nn.GELU(),\n",
    "            torch.nn.Dropout(dropout),\n",
    "            torch.nn.Linear(ff_dim, hidden_dim),\n",
    "            torch.nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Self attention + residual connection\n",
    "        attn_output = self.attention(x)\n",
    "        x = self.norm1(x + attn_output)\n",
    "        \n",
    "        # Feed forward + residual connection\n",
    "        ff_output = self.ff(x)\n",
    "        x = self.norm2(x + ff_output)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# TransformerGCN model\n",
    "class TransformerGCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers=3, \n",
    "                 num_heads=4, ff_dim_factor=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = torch.nn.Linear(in_channels, hidden_channels)\n",
    "        \n",
    "        # GCN Layers\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            self.convs.append(GCNConv(hidden_channels, hidden_channels))\n",
    "        \n",
    "        # Transformer blocks (one after each GCN layer)\n",
    "        self.transformers = torch.nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            self.transformers.append(\n",
    "                TransformerBlock(hidden_channels, num_heads, hidden_channels * ff_dim_factor, dropout)\n",
    "            )\n",
    "        \n",
    "        # Output layers\n",
    "        self.fc1 = torch.nn.Linear(hidden_channels, hidden_channels // 2)\n",
    "        self.fc2 = torch.nn.Linear(hidden_channels // 2, out_channels)\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        # Save for reshaping in forward\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, batch):\n",
    "        # Initial embedding\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # Apply GCN and Transformer layers\n",
    "        for i, (conv, transformer) in enumerate(zip(self.convs, self.transformers)):\n",
    "            # GCN Layer\n",
    "            x_conv = conv(x, edge_index)\n",
    "            x_conv = F.relu(x_conv)\n",
    "            x_conv = F.dropout(x_conv, p=self.dropout, training=self.training)\n",
    "            \n",
    "            # Reshape for transformer (using a safer approach)\n",
    "            # Create a padded tensor per batch\n",
    "            batch_idx = torch.unique(batch)\n",
    "            transformed_features = []\n",
    "            \n",
    "            for b in batch_idx:\n",
    "                # Get nodes for this batch\n",
    "                mask = (batch == b)\n",
    "                nodes_batch = x_conv[mask]\n",
    "                \n",
    "                # Process through transformer (add batch dimension)\n",
    "                if nodes_batch.size(0) > 0:  # Skip empty batches\n",
    "                    # Add batch and sequence dimensions\n",
    "                    nodes_batch = nodes_batch.unsqueeze(0)\n",
    "                    \n",
    "                    # Apply transformer\n",
    "                    transformed = transformer(nodes_batch)\n",
    "                    \n",
    "                    # Remove batch dimension\n",
    "                    transformed = transformed.squeeze(0)\n",
    "                    transformed_features.append(transformed)\n",
    "            \n",
    "            # Check if we have any valid features\n",
    "            if transformed_features:\n",
    "                # Concatenate all transformed features\n",
    "                x = torch.cat(transformed_features, dim=0)\n",
    "            else:\n",
    "                # Fallback - shouldn't typically happen\n",
    "                x = x_conv\n",
    "        \n",
    "        # Global pooling\n",
    "        x = global_mean_pool(x, batch)\n",
    "        \n",
    "        # MLP for final prediction\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Training and AUC\n",
    "def train(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in loader:\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index, data.edge_attr, data.batch).squeeze()\n",
    "        loss = criterion(torch.sigmoid(out), data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def compute_auc(model, loader):\n",
    "    model.eval()\n",
    "    y_true, y_probs = [], []\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            out = model(data.x, data.edge_index, data.edge_attr, data.batch).squeeze()\n",
    "            y_probs.extend(torch.sigmoid(out).cpu().numpy())\n",
    "            y_true.extend(data.y.cpu().numpy())\n",
    "    return roc_auc_score(y_true, y_probs)\n",
    "\n",
    "# Model evaluation\n",
    "def get_metrics(model, loader):\n",
    "    model.eval()\n",
    "    y_true, y_probs = [], []\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            out = model(data.x, data.edge_index, data.edge_attr, data.batch).squeeze()\n",
    "            y_probs.extend(torch.sigmoid(out).cpu().numpy())\n",
    "            y_true.extend(data.y.cpu().numpy())\n",
    "    y_preds = (np.array(y_probs) > 0.5).astype(int)\n",
    "    return {\n",
    "        'AUC': roc_auc_score(y_true, y_probs),\n",
    "        'Accuracy': accuracy_score(y_true, y_preds),\n",
    "        'MCC': matthews_corrcoef(y_true, y_preds),\n",
    "        'Precision': precision_score(y_true, y_preds),\n",
    "        'F1': f1_score(y_true, y_preds),\n",
    "        'Kappa': cohen_kappa_score(y_true, y_preds),\n",
    "        'Brier': brier_score_loss(y_true, y_probs),\n",
    "    }\n",
    "\n",
    "# Globals for Optuna\n",
    "trial_train_loader = None\n",
    "trial_val_loader = None\n",
    "\n",
    "# Optuna objective\n",
    "def objective(trial):\n",
    "    # Ensure hidden_channels is divisible by all possible num_heads values\n",
    "    hidden_channels = trial.suggest_int(\"hidden_channels\", 32, 128, step=16)  # Ensures divisibility\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 2, 4)\n",
    "    num_heads = trial.suggest_int(\"num_heads\", 2, 8, step=2)  # Use only even numbers\n",
    "    ff_dim_factor = trial.suggest_int(\"ff_dim_factor\", 2, 4)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)\n",
    "\n",
    "    model = TransformerGCN(20, hidden_channels, 1, num_layers, num_heads, ff_dim_factor, dropout)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = torch.nn.BCELoss()\n",
    "\n",
    "    for epoch in range(10):\n",
    "        train(model, trial_train_loader, optimizer, criterion)\n",
    "\n",
    "    return compute_auc(model, trial_val_loader)\n",
    "\n",
    "def main():\n",
    "    global trial_train_loader, trial_val_loader\n",
    "\n",
    "    graphs = load_dataset(\"dataset_main.xlsx\")\n",
    "    train_data, test_data = train_test_split(graphs, test_size=0.2, random_state=42)\n",
    "    trial_train, trial_val = train_test_split(train_data, test_size=0.2, random_state=42)\n",
    "    trial_train_loader = DataLoader(trial_train, batch_size=32, shuffle=True)\n",
    "    trial_val_loader = DataLoader(trial_val, batch_size=32)\n",
    "\n",
    "    # Optuna tuning\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=30)\n",
    "    best_params = study.best_trial.params\n",
    "    json.dump(best_params, open(os.path.join(output_dir, \"study_best_params_transformer_gcn.json\"), \"w\"), indent=4)\n",
    "\n",
    "    # 10-fold cross-validation (mean metrics only)\n",
    "    kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "    metrics_list = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(train_data), start=1):\n",
    "        model = TransformerGCN(\n",
    "            20, best_params[\"hidden_channels\"], 1, \n",
    "            best_params[\"num_layers\"], best_params[\"num_heads\"],\n",
    "            best_params[\"ff_dim_factor\"], best_params[\"dropout\"]\n",
    "        )\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=best_params[\"lr\"])\n",
    "        criterion = torch.nn.BCELoss()\n",
    "\n",
    "        train_fold = [train_data[i] for i in train_idx]\n",
    "        val_fold = [train_data[i] for i in val_idx]\n",
    "        loader_train = DataLoader(train_fold, batch_size=32, shuffle=True)\n",
    "        loader_val = DataLoader(val_fold, batch_size=32)\n",
    "\n",
    "        for epoch in range(30):\n",
    "            train(model, loader_train, optimizer, criterion)\n",
    "\n",
    "        metrics = get_metrics(model, loader_val)\n",
    "        metrics_list.append(metrics)\n",
    "\n",
    "    # Save and print mean of cross-validation metrics\n",
    "    df_cv = pd.DataFrame(metrics_list)\n",
    "    mean_metrics = df_cv.mean()\n",
    "    df_cv.loc['mean'] = mean_metrics\n",
    "    df_cv.tail(1).to_excel(os.path.join(output_dir, \"cv_metrics_mean.xlsx\"), index=False)\n",
    "\n",
    "    print(\"\\n📊 10-Fold Cross-Validation Mean Metrics:\")\n",
    "    for key, value in mean_metrics.items():\n",
    "        print(f\"{key}: {value:.4f}\")\n",
    "\n",
    "    # Final training on full training set\n",
    "    full_train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(test_data, batch_size=32)\n",
    "\n",
    "    final_model = TransformerGCN(\n",
    "        20, best_params[\"hidden_channels\"], 1, \n",
    "        best_params[\"num_layers\"], best_params[\"num_heads\"],\n",
    "        best_params[\"ff_dim_factor\"], best_params[\"dropout\"]\n",
    "    )\n",
    "    optimizer = torch.optim.Adam(final_model.parameters(), lr=best_params[\"lr\"])\n",
    "    criterion = torch.nn.BCELoss()\n",
    "\n",
    "    losses, aucs = [], []\n",
    "    for epoch in range(1, 101):\n",
    "        loss = train(final_model, full_train_loader, optimizer, criterion)\n",
    "        auc = compute_auc(final_model, full_train_loader)\n",
    "        losses.append(loss)\n",
    "        aucs.append(auc)\n",
    "        print(f\"[Final Train] Epoch {epoch} | Loss: {loss:.4f} | AUC: {auc:.4f}\")\n",
    "\n",
    "    torch.save(final_model.state_dict(), os.path.join(output_dir, \"transformer_gcn_best_model.pth\"))\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(losses, label=\"Loss\")\n",
    "    plt.title(\"Final Training Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.savefig(os.path.join(output_dir, \"final_training_loss.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(aucs, label=\"AUC\", color='red')\n",
    "    plt.title(\"Final Training AUC\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"AUC\")\n",
    "    plt.savefig(os.path.join(output_dir, \"final_training_auc.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # Final test set evaluation\n",
    "    test_metrics = get_metrics(final_model, test_loader)\n",
    "    pd.DataFrame([test_metrics]).to_excel(os.path.join(output_dir, \"test_metrics.xlsx\"), index=False)\n",
    "\n",
    "    print(\"\\n🧪 Final Test Set Metrics:\")\n",
    "    for key, value in test_metrics.items():\n",
    "        print(f\"{key}: {value:.4f}\")\n",
    "\n",
    "    # ROC Curve\n",
    "    model = final_model\n",
    "    model.eval()\n",
    "    y_true, y_probs = [], []\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            out = model(data.x, data.edge_index, data.edge_attr, data.batch).squeeze()\n",
    "            y_probs.extend(torch.sigmoid(out).cpu().numpy())\n",
    "            y_true.extend(data.y.cpu().numpy())\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_probs)\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, label=f\"AUC = {test_metrics['AUC']:.2f}\")\n",
    "    plt.plot([0, 1], [0, 1], '--', color='gray')\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"Test ROC Curve\")\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(output_dir, \"test_roc.png\"))\n",
    "    plt.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05c572f-8876-4a69-8542-d08485ca880d",
   "metadata": {},
   "outputs": [],
   "source": [
    "{'hidden_channels': 80, 'num_layers': 2, 'num_heads': 8, 'ff_dim_factor': 4, 'dropout': 0.22570405839644114, 'lr': 0.00010100422122258602}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c792fc-23fd-4201-a421-57a2a0b70615",
   "metadata": {},
   "source": [
    "## Confusion matrix generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8612af46-f11f-4505-909d-462c8628404e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from torch_geometric.data import DataLoader, Data\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import rdMolDescriptors\n",
    "\n",
    "# Reuse atom features function from cell1\n",
    "def atom_features(atom):\n",
    "    return torch.tensor([\n",
    "        # Basic properties\n",
    "        atom.GetAtomicNum(),                     # Atomic number\n",
    "        atom.GetDegree(),                        # Number of bonded neighbors\n",
    "        atom.GetFormalCharge(),                  # Formal charge\n",
    "        atom.GetNumRadicalElectrons(),           # Number of radical electrons\n",
    "        int(atom.GetIsAromatic()),               # Aromaticity flag\n",
    "        \n",
    "        # Extended properties\n",
    "        atom.GetExplicitValence(),               # Explicit valence\n",
    "        atom.GetImplicitValence(),               # Implicit valence\n",
    "        atom.GetTotalValence(),                  # Total valence\n",
    "        atom.GetNumImplicitHs(),                 # Number of implicit hydrogens\n",
    "        atom.GetHybridization(),                 # Hybridization state\n",
    "        atom.GetTotalNumHs(),                    # Total number of hydrogens\n",
    "        \n",
    "        # Topological properties\n",
    "        int(atom.IsInRing()),                    # Whether the atom is in a ring\n",
    "        int(atom.IsInRingSize(3)),               # Whether in 3-membered ring\n",
    "        int(atom.IsInRingSize(4)),               # Whether in 4-membered ring\n",
    "        int(atom.IsInRingSize(5)),               # Whether in 5-membered ring\n",
    "        int(atom.IsInRingSize(6)),               # Whether in 6-membered ring\n",
    "        int(atom.IsInRingSize(7)),               # Whether in 7-membered ring\n",
    "        \n",
    "        # Electronic properties\n",
    "        atom.GetChiralTag(),                     # Chirality\n",
    "        atom.GetMass(),                          # Atomic mass\n",
    "        Chem.rdMolDescriptors.CalcCrippenDescriptors(\n",
    "            Chem.MolFromSmiles(f\"[{atom.GetSymbol()}]\"))[0]  # LogP contribution\n",
    "    ], dtype=torch.float)\n",
    "\n",
    "def mol_to_graph(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None: return None\n",
    "    atoms = [atom_features(atom) for atom in mol.GetAtoms()]\n",
    "    if not atoms: return None\n",
    "    x = torch.stack(atoms, dim=0)\n",
    "    edge_index = []\n",
    "    for bond in mol.GetBonds():\n",
    "        i, j = bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()\n",
    "        edge_index += [[i, j], [j, i]]\n",
    "    edge_index = torch.tensor(edge_index).t().contiguous()\n",
    "    edge_attr = [[bond.GetBondTypeAsDouble(), bond.GetIsConjugated(), bond.IsInRing()] * 2 for bond in mol.GetBonds()]\n",
    "    edge_attr = torch.tensor(edge_attr, dtype=torch.float).reshape(-1, 3)\n",
    "    return Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "\n",
    "def load_dataset(path, smiles_col='Smiles', target_col='Target'):\n",
    "    df = pd.read_excel(path)\n",
    "    graphs = []\n",
    "    for _, row in df.iterrows():\n",
    "        if pd.isna(row[smiles_col]) or pd.isna(row[target_col]):\n",
    "            continue\n",
    "        g = mol_to_graph(str(row[smiles_col]))\n",
    "        if g is not None:\n",
    "            g.y = torch.tensor([row[target_col]], dtype=torch.float)\n",
    "            g.smiles = row[smiles_col]\n",
    "            graphs.append(g)\n",
    "    return graphs\n",
    "\n",
    "# Import TransformerGCN architecture from cell1\n",
    "class MultiHeadAttention(torch.nn.Module):\n",
    "    def __init__(self, hidden_dim, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        # Ensure hidden_dim is divisible by num_heads\n",
    "        # If not, we'll adjust the head_dim to make it work\n",
    "        self.head_dim = hidden_dim // num_heads\n",
    "        \n",
    "        # Actual dimension after ensuring divisibility\n",
    "        self.actual_dim = self.head_dim * num_heads\n",
    "        \n",
    "        self.query = torch.nn.Linear(hidden_dim, self.actual_dim)\n",
    "        self.key = torch.nn.Linear(hidden_dim, self.actual_dim)\n",
    "        self.value = torch.nn.Linear(hidden_dim, self.actual_dim)\n",
    "        self.proj = torch.nn.Linear(self.actual_dim, hidden_dim)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        seq_length = x.size(1)\n",
    "        \n",
    "        # Linear projections\n",
    "        q = self.query(x).view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k = self.key(x).view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        v = self.value(x).view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # Scaled dot-product attention\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "        \n",
    "        # Apply attention weights\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        context = torch.matmul(attn_weights, v)\n",
    "        \n",
    "        # Concatenate heads and put through final linear layer\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, seq_length, self.actual_dim)\n",
    "        output = self.proj(context)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Transformer Block\n",
    "class TransformerBlock(torch.nn.Module):\n",
    "    def __init__(self, hidden_dim, num_heads, ff_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(hidden_dim, num_heads, dropout)\n",
    "        self.norm1 = torch.nn.LayerNorm(hidden_dim)\n",
    "        self.norm2 = torch.nn.LayerNorm(hidden_dim)\n",
    "        self.ff = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_dim, ff_dim),\n",
    "            torch.nn.GELU(),\n",
    "            torch.nn.Dropout(dropout),\n",
    "            torch.nn.Linear(ff_dim, hidden_dim),\n",
    "            torch.nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Self attention + residual connection\n",
    "        attn_output = self.attention(x)\n",
    "        x = self.norm1(x + attn_output)\n",
    "        \n",
    "        # Feed forward + residual connection\n",
    "        ff_output = self.ff(x)\n",
    "        x = self.norm2(x + ff_output)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# TransformerGCN model\n",
    "class TransformerGCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers=3, \n",
    "                 num_heads=4, ff_dim_factor=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = torch.nn.Linear(in_channels, hidden_channels)\n",
    "        \n",
    "        # GCN Layers\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            self.convs.append(GCNConv(hidden_channels, hidden_channels))\n",
    "        \n",
    "        # Transformer blocks (one after each GCN layer)\n",
    "        self.transformers = torch.nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            self.transformers.append(\n",
    "                TransformerBlock(hidden_channels, num_heads, hidden_channels * ff_dim_factor, dropout)\n",
    "            )\n",
    "        \n",
    "        # Output layers\n",
    "        self.fc1 = torch.nn.Linear(hidden_channels, hidden_channels // 2)\n",
    "        self.fc2 = torch.nn.Linear(hidden_channels // 2, out_channels)\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        # Save for reshaping in forward\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, batch):\n",
    "        # Initial embedding\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # Apply GCN and Transformer layers\n",
    "        for i, (conv, transformer) in enumerate(zip(self.convs, self.transformers)):\n",
    "            # GCN Layer\n",
    "            x_conv = conv(x, edge_index)\n",
    "            x_conv = F.relu(x_conv)\n",
    "            x_conv = F.dropout(x_conv, p=self.dropout, training=self.training)\n",
    "            \n",
    "            # Reshape for transformer (using a safer approach)\n",
    "            # Create a padded tensor per batch\n",
    "            batch_idx = torch.unique(batch)\n",
    "            transformed_features = []\n",
    "            \n",
    "            for b in batch_idx:\n",
    "                # Get nodes for this batch\n",
    "                mask = (batch == b)\n",
    "                nodes_batch = x_conv[mask]\n",
    "                \n",
    "                # Process through transformer (add batch dimension)\n",
    "                if nodes_batch.size(0) > 0:  # Skip empty batches\n",
    "                    # Add batch and sequence dimensions\n",
    "                    nodes_batch = nodes_batch.unsqueeze(0)\n",
    "                    \n",
    "                    # Apply transformer\n",
    "                    transformed = transformer(nodes_batch)\n",
    "                    \n",
    "                    # Remove batch dimension\n",
    "                    transformed = transformed.squeeze(0)\n",
    "                    transformed_features.append(transformed)\n",
    "            \n",
    "            # Check if we have any valid features\n",
    "            if transformed_features:\n",
    "                # Concatenate all transformed features\n",
    "                x = torch.cat(transformed_features, dim=0)\n",
    "            else:\n",
    "                # Fallback - shouldn't typically happen\n",
    "                x = x_conv\n",
    "        \n",
    "        # Global pooling\n",
    "        x = global_mean_pool(x, batch)\n",
    "        \n",
    "        # MLP for final prediction\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Main execution function for generating confusion matrix\n",
    "def generate_confusion_matrix():\n",
    "    # Use the correct output directory from cell1 for TransformerGCN\n",
    "    output_dir = 'Transformer_GCN'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Load the best parameters for TransformerGCN\n",
    "    try:\n",
    "        with open(os.path.join(output_dir, \"study_best_params_transformer_gcn.json\"), \"r\") as f:\n",
    "            best_params = json.load(f)\n",
    "        print(\"Best parameters loaded successfully!\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Best parameters file not found. Using default parameters.\")\n",
    "        best_params = {\n",
    "            \"hidden_channels\": 64,\n",
    "            \"num_layers\": 3,\n",
    "            \"num_heads\": 4,\n",
    "            \"ff_dim_factor\": 4,\n",
    "            \"dropout\": 0.1,\n",
    "            \"lr\": 0.001\n",
    "        }\n",
    "    \n",
    "    # Load dataset and create test loader\n",
    "    graphs = load_dataset(\"dataset_main.xlsx\")\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    _, test_data = train_test_split(graphs, test_size=0.2, random_state=42)\n",
    "    test_loader = DataLoader(test_data, batch_size=32)\n",
    "    \n",
    "    # Initialize TransformerGCN model with best parameters\n",
    "    model = TransformerGCN(\n",
    "        20, \n",
    "        best_params[\"hidden_channels\"], \n",
    "        1, \n",
    "        best_params[\"num_layers\"], \n",
    "        best_params[\"num_heads\"],\n",
    "        best_params[\"ff_dim_factor\"], \n",
    "        best_params[\"dropout\"]\n",
    "    )\n",
    "    \n",
    "    # Load trained model weights\n",
    "    try:\n",
    "        model.load_state_dict(torch.load(os.path.join(output_dir, \"transformer_gcn_best_model.pth\")))\n",
    "        print(\"Model loaded successfully!\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Model file not found. Cannot generate confusion matrix without trained model.\")\n",
    "        return\n",
    "    \n",
    "    # Generate predictions\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    y_probs = []\n",
    "    molecule_smiles = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            out = model(data.x, data.edge_index, data.edge_attr, data.batch).squeeze()\n",
    "            probs = torch.sigmoid(out).cpu().numpy()\n",
    "            preds = (probs > 0.5).astype(int)\n",
    "            \n",
    "            y_probs.extend(probs)\n",
    "            y_pred.extend(preds)\n",
    "            y_true.extend(data.y.cpu().numpy())\n",
    "            molecule_smiles.extend([g.smiles for g in data.to_data_list()])\n",
    "    \n",
    "    # Create confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    class_names = ['Negative', 'Positive']\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
    "    disp.plot(cmap=plt.cm.Blues, values_format='d')\n",
    "    plt.title('Confusion Matrix for TransformerGCN Model')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, \"confusion_matrix.png\"))\n",
    "    \n",
    "    # Create a prettier confusion matrix with seaborn\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.title('Confusion Matrix for TransformerGCN Model')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, \"confusion_matrix_seaborn.png\"))\n",
    "    \n",
    "    # Create a dataframe with predictions for analysis\n",
    "    results_df = pd.DataFrame({\n",
    "        'SMILES': molecule_smiles,\n",
    "        'True_Label': y_true,\n",
    "        'Predicted_Label': y_pred,\n",
    "        'Prediction_Probability': y_probs\n",
    "    })\n",
    "    \n",
    "    # Save predictions to Excel\n",
    "    results_df.to_excel(os.path.join(output_dir, \"transformer_gcn_prediction_results.xlsx\"), index=False)\n",
    "    \n",
    "    # Calculate metrics for each class\n",
    "    TN, FP, FN, TP = cm.ravel()\n",
    "    \n",
    "    # True Positive Rate (Sensitivity or Recall)\n",
    "    TPR = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "    \n",
    "    # True Negative Rate (Specificity)\n",
    "    TNR = TN / (TN + FP) if (TN + FP) > 0 else 0\n",
    "    \n",
    "    # False Positive Rate\n",
    "    FPR = FP / (FP + TN) if (FP + TN) > 0 else 0\n",
    "    \n",
    "    # False Negative Rate\n",
    "    FNR = FN / (TP + FN) if (TP + FN) > 0 else 0\n",
    "    \n",
    "    # Precision\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "    \n",
    "    # F1 Score\n",
    "    f1 = 2 * TP / (2 * TP + FP + FN) if (2 * TP + FP + FN) > 0 else 0\n",
    "    \n",
    "    # Print detailed metrics\n",
    "    print(\"\\n===== Confusion Matrix =====\")\n",
    "    print(f\"True Negative (TN): {TN}\")\n",
    "    print(f\"False Positive (FP): {FP}\")\n",
    "    print(f\"False Negative (FN): {FN}\")\n",
    "    print(f\"True Positive (TP): {TP}\")\n",
    "    print(\"\\n===== Detailed Metrics =====\")\n",
    "    print(f\"True Positive Rate (Sensitivity/Recall): {TPR:.4f}\")\n",
    "    print(f\"True Negative Rate (Specificity): {TNR:.4f}\")\n",
    "    print(f\"False Positive Rate: {FPR:.4f}\")\n",
    "    print(f\"False Negative Rate: {FNR:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    \n",
    "    # Save metrics to file\n",
    "    metrics_df = pd.DataFrame({\n",
    "        'Metric': ['TN', 'FP', 'FN', 'TP', 'Sensitivity/Recall', 'Specificity', \n",
    "                  'False Positive Rate', 'False Negative Rate', 'Precision', 'F1 Score'],\n",
    "        'Value': [TN, FP, FN, TP, TPR, TNR, FPR, FNR, precision, f1]\n",
    "    })\n",
    "    metrics_df.to_excel(os.path.join(output_dir, \"transformer_gcn_detailed_metrics.xlsx\"), index=False)\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# Run the function\n",
    "if __name__ == \"__main__\":\n",
    "    generate_confusion_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014b0d1b-4a61-4a42-9ad1-79ef3cbdb21e",
   "metadata": {},
   "source": [
    "# External Dataset Evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3eb3ee0-f491-43a6-afd0-b85ce507cc27",
   "metadata": {},
   "source": [
    "## Phytos_Results.xlsx\n",
    "- Predicted Results saved to excel\n",
    "- ROC plot and Confusion Matrix plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973a8338-2ff3-4360-9ac4-6613e3a9d83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: External Dataset Evaluation with Transformer GCN Model\n",
    "\n",
    "# Import additional required libraries\n",
    "import seaborn as sns\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "import warnings\n",
    "\n",
    "# Ensure output directory exists\n",
    "output_dir = 'Transformer_GCN'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# First, examine the saved model to understand its architecture\n",
    "saved_state_dict = torch.load(os.path.join(output_dir, \"transformer_gcn_best_model.pth\"))\n",
    "\n",
    "# Print the keys to see the model structure\n",
    "print(\"Saved model layers:\")\n",
    "for key in saved_state_dict.keys():\n",
    "    if 'weight' in key:\n",
    "        print(f\"{key}: {saved_state_dict[key].shape}\")\n",
    "\n",
    "# Function to load external dataset\n",
    "def load_external_dataset(path, smiles_col='Smiles', target_col='Target'):\n",
    "    df = pd.read_excel(path)\n",
    "    graphs = []\n",
    "    molecules = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        if pd.isna(row[smiles_col]):\n",
    "            continue\n",
    "            \n",
    "        g = mol_to_graph(str(row[smiles_col]))\n",
    "        if g is not None:\n",
    "            # If target is available, use it, otherwise set to None\n",
    "            if target_col in df.columns and not pd.isna(row[target_col]):\n",
    "                g.y = torch.tensor([row[target_col]], dtype=torch.float)\n",
    "            else:\n",
    "                g.y = None\n",
    "                \n",
    "            g.smiles = row[smiles_col]\n",
    "            graphs.append(g)\n",
    "            molecules.append(row.to_dict())\n",
    "    \n",
    "    return graphs, pd.DataFrame(molecules)\n",
    "\n",
    "# Custom model class that matches the saved TransformerGCN architecture\n",
    "class TransformerGCNForLoading(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers=3, \n",
    "                 num_heads=4, ff_dim_factor=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = torch.nn.Linear(in_channels, hidden_channels)\n",
    "        \n",
    "        # GCN Layers\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            self.convs.append(GCNConv(hidden_channels, hidden_channels))\n",
    "        \n",
    "        # Transformer blocks (one after each GCN layer)\n",
    "        self.transformers = torch.nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            self.transformers.append(\n",
    "                TransformerBlock(hidden_channels, num_heads, hidden_channels * ff_dim_factor, dropout)\n",
    "            )\n",
    "        \n",
    "        # Output layers\n",
    "        self.fc1 = torch.nn.Linear(hidden_channels, hidden_channels // 2)\n",
    "        self.fc2 = torch.nn.Linear(hidden_channels // 2, out_channels)\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        # Save for reshaping in forward\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, batch):\n",
    "        # Initial embedding\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # Apply GCN and Transformer layers\n",
    "        for i, (conv, transformer) in enumerate(zip(self.convs, self.transformers)):\n",
    "            # GCN Layer\n",
    "            x_conv = conv(x, edge_index)\n",
    "            x_conv = F.relu(x_conv)\n",
    "            x_conv = F.dropout(x_conv, p=self.dropout, training=self.training)\n",
    "            \n",
    "            # Reshape for transformer (using a safer approach)\n",
    "            # Create a padded tensor per batch\n",
    "            batch_idx = torch.unique(batch)\n",
    "            transformed_features = []\n",
    "            \n",
    "            for b in batch_idx:\n",
    "                # Get nodes for this batch\n",
    "                mask = (batch == b)\n",
    "                nodes_batch = x_conv[mask]\n",
    "                \n",
    "                # Process through transformer (add batch dimension)\n",
    "                if nodes_batch.size(0) > 0:  # Skip empty batches\n",
    "                    # Add batch and sequence dimensions\n",
    "                    nodes_batch = nodes_batch.unsqueeze(0)\n",
    "                    \n",
    "                    # Apply transformer\n",
    "                    transformed = transformer(nodes_batch)\n",
    "                    \n",
    "                    # Remove batch dimension\n",
    "                    transformed = transformed.squeeze(0)\n",
    "                    transformed_features.append(transformed)\n",
    "            \n",
    "            # Check if we have any valid features\n",
    "            if transformed_features:\n",
    "                # Concatenate all transformed features\n",
    "                x = torch.cat(transformed_features, dim=0)\n",
    "            else:\n",
    "                # Fallback - shouldn't typically happen\n",
    "                x = x_conv\n",
    "        \n",
    "        # Global pooling\n",
    "        x = global_mean_pool(x, batch)\n",
    "        \n",
    "        # MLP for final prediction\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Determine model parameters from saved state dict\n",
    "# We need to load the best hyperparameters from the JSON file\n",
    "try:\n",
    "    best_params = json.load(open(os.path.join(output_dir, \"study_best_params_transformer_gcn.json\"), \"r\"))\n",
    "    hidden_channels = best_params[\"hidden_channels\"]\n",
    "    num_layers = best_params[\"num_layers\"]\n",
    "    num_heads = best_params[\"num_heads\"]\n",
    "    ff_dim_factor = best_params[\"ff_dim_factor\"]\n",
    "    dropout = best_params[\"dropout\"]\n",
    "    print(f\"Loaded parameters: hidden_channels={hidden_channels}, num_layers={num_layers}, \"\n",
    "          f\"num_heads={num_heads}, ff_dim_factor={ff_dim_factor}, dropout={dropout}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading hyperparameters: {str(e)}\")\n",
    "    # Default values if JSON loading fails\n",
    "    hidden_channels = 64\n",
    "    num_layers = 3\n",
    "    num_heads = 4\n",
    "    ff_dim_factor = 4\n",
    "    dropout = 0.1\n",
    "    print(f\"Using default parameters: hidden_channels={hidden_channels}, num_layers={num_layers}, \"\n",
    "          f\"num_heads={num_heads}, ff_dim_factor={ff_dim_factor}, dropout={dropout}\")\n",
    "\n",
    "# Create the model with the correct architecture\n",
    "model = TransformerGCNForLoading(\n",
    "    in_channels=20,  # Using 20 input features as in Cell 1\n",
    "    hidden_channels=hidden_channels, \n",
    "    out_channels=1,\n",
    "    num_layers=num_layers,\n",
    "    num_heads=num_heads,\n",
    "    ff_dim_factor=ff_dim_factor,\n",
    "    dropout=dropout\n",
    ")\n",
    "\n",
    "# Try to load the saved state\n",
    "try:\n",
    "    model.load_state_dict(torch.load(os.path.join(output_dir, \"transformer_gcn_best_model.pth\")))\n",
    "    print(\"Successfully loaded model weights\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model weights: {str(e)}\")\n",
    "    print(\"This may happen if the model architecture doesn't match the saved weights.\")\n",
    "    print(\"Make sure you've trained the model with the updated architecture first.\")\n",
    "    # Continue anyway for demonstration purposes\n",
    "    print(\"Continuing with uninitialized model for demonstration...\")\n",
    "\n",
    "# Set to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Load external dataset\n",
    "external_data, external_df = load_external_dataset(\"Phytos_results.xlsx\")\n",
    "# Suppress deprecation warning for DataLoader\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "    external_loader = DataLoader(external_data, batch_size=32)\n",
    "\n",
    "# Get predictions\n",
    "y_true = []\n",
    "y_probs = []\n",
    "smiles_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in external_loader:\n",
    "        try:\n",
    "            out = model(batch.x, batch.edge_index, batch.edge_attr, batch.batch)\n",
    "            # Don't squeeze - handle the dimensionality properly\n",
    "            probs = torch.sigmoid(out).cpu().numpy()\n",
    "            \n",
    "            # Handle both single-item and multi-item batches\n",
    "            if len(probs.shape) == 0:  # Single item case (0-d array)\n",
    "                probs = np.array([probs.item()])\n",
    "            \n",
    "            for i in range(len(probs)):\n",
    "                y_probs.append(probs[i][0] if probs[i].size > 1 else probs[i])\n",
    "                \n",
    "                # Get the SMILES safely\n",
    "                if hasattr(batch, 'smiles'):\n",
    "                    if isinstance(batch.smiles, list):\n",
    "                        smiles_list.append(batch.smiles[i])\n",
    "                    else:\n",
    "                        # Handle the case where smiles might be a single string\n",
    "                        smiles_list.append(batch.smiles)\n",
    "                \n",
    "                # Get the true value if it exists\n",
    "                if hasattr(batch, 'y') and batch.y is not None:\n",
    "                    if batch.y.dim() > 0 and i < batch.y.size(0):\n",
    "                        y_true.append(batch.y[i].item())\n",
    "                    elif batch.y.dim() == 0:  # Single item\n",
    "                        y_true.append(batch.y.item())\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing batch: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "# Save predictions to Excel\n",
    "results_df = pd.DataFrame({\n",
    "    'Smiles': smiles_list,\n",
    "    'Predicted_Probability': y_probs,\n",
    "    'Predicted_Class': [1 if p > 0.5 else 0 for p in y_probs]\n",
    "})\n",
    "\n",
    "# If we have true values, add them and calculate metrics\n",
    "if len(y_true) > 0 and len(y_true) == len(y_probs):\n",
    "    results_df['Actual'] = y_true\n",
    "    \n",
    "    # Calculate metrics\n",
    "    y_true_np = np.array(y_true)  # Convert list to numpy array\n",
    "    y_probs_np = np.array(y_probs)\n",
    "    y_preds = (y_probs_np > 0.5).astype(int)\n",
    "    \n",
    "    # Check if we have multiple classes for metrics that require it\n",
    "    unique_classes = np.unique(y_true_np)\n",
    "    has_multiple_classes = len(unique_classes) > 1\n",
    "    \n",
    "    metrics = {\n",
    "        'Accuracy': accuracy_score(y_true_np, y_preds),\n",
    "        'Precision': precision_score(y_true_np, y_preds, zero_division=0),\n",
    "        'F1': f1_score(y_true_np, y_preds, zero_division=0),\n",
    "        'Kappa': cohen_kappa_score(y_true_np, y_preds),\n",
    "        'Brier': brier_score_loss(y_true_np, y_probs_np),\n",
    "    }\n",
    "    \n",
    "    # Only calculate AUC and MCC if we have multiple classes\n",
    "    if has_multiple_classes:\n",
    "        # Suppress the UndefinedMetricWarning\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "            metrics['AUC'] = roc_auc_score(y_true_np, y_probs_np)\n",
    "            metrics['MCC'] = matthews_corrcoef(y_true_np, y_preds)\n",
    "    else:\n",
    "        print(\"\\n⚠️ Warning: Only one class is present in the dataset.\")\n",
    "        print(\"AUC and MCC are not defined in this case and will be reported as N/A.\")\n",
    "        metrics['AUC'] = \"N/A\"\n",
    "        metrics['MCC'] = \"N/A\"\n",
    "    \n",
    "    # Convert N/A values to NaN for Excel export\n",
    "    metrics_for_excel = {k: np.nan if v == \"N/A\" else v for k, v in metrics.items()}\n",
    "    \n",
    "    # Save metrics\n",
    "    pd.DataFrame([metrics_for_excel]).to_excel(os.path.join(output_dir, \"external_metrics.xlsx\"), index=False)\n",
    "    \n",
    "    print(\"\\n🧪 External Dataset Metrics:\")\n",
    "    for key, value in metrics.items():\n",
    "        if isinstance(value, str):\n",
    "            print(f\"{key}: {value}\")\n",
    "        else:\n",
    "            print(f\"{key}: {value:.4f}\")\n",
    "    \n",
    "    # Only generate ROC curve if we have multiple classes\n",
    "    if has_multiple_classes:\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "            fpr, tpr, _ = roc_curve(y_true_np, y_probs_np)\n",
    "            \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.plot(fpr, tpr, label=f\"AUC = {metrics['AUC']:.2f}\" if isinstance(metrics['AUC'], float) else \"AUC = N/A\")\n",
    "        plt.plot([0, 1], [0, 1], '--', color='gray')\n",
    "        plt.xlabel(\"False Positive Rate\")\n",
    "        plt.ylabel(\"True Positive Rate\")\n",
    "        plt.title(\"External Dataset ROC Curve\")\n",
    "        plt.legend()\n",
    "        plt.savefig(os.path.join(output_dir, \"external_roc.png\"))\n",
    "        plt.close()\n",
    "    \n",
    "    # Create confusion matrix\n",
    "    cm = confusion_matrix(y_true_np, y_preds)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.title('Confusion Matrix - External Dataset')\n",
    "    plt.savefig(os.path.join(output_dir, \"external_confusion_matrix.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # Class distribution information\n",
    "    class_counts = np.bincount(y_true_np.astype(int))\n",
    "    print(\"\\n📊 Class Distribution in External Dataset:\")\n",
    "    for i, count in enumerate(class_counts):\n",
    "        print(f\"Class {i}: {count} samples ({count/len(y_true_np)*100:.1f}%)\")\n",
    "\n",
    "# Save all predictions\n",
    "results_df.to_excel(os.path.join(output_dir, \"external_predictions.xlsx\"), index=False)\n",
    "print(f\"\\n💾 Predictions for {len(results_df)} molecules saved to {os.path.join(output_dir, 'external_predictions.xlsx')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d7baa0-8bb4-4c8f-b0dc-ccea217af5b9",
   "metadata": {},
   "source": [
    "## Cancer_set.xlsx\n",
    "- Predicted Results saved to excel\n",
    "- ROC plot \n",
    "- Confusion Matrix plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef5be28-642d-48d4-9af2-44a7e27acfdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: External Dataset Evaluation with Transformer GCN Model\n",
    "\n",
    "# Import additional required libraries\n",
    "import seaborn as sns\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "import warnings\n",
    "\n",
    "# Ensure output directory exists\n",
    "output_dir = 'Transformer_GCN'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# First, examine the saved model to understand its architecture\n",
    "saved_state_dict = torch.load(os.path.join(output_dir, \"transformer_gcn_best_model.pth\"))\n",
    "\n",
    "# Print the keys to see the model structure\n",
    "print(\"Saved model layers:\")\n",
    "for key in saved_state_dict.keys():\n",
    "    if 'weight' in key:\n",
    "        print(f\"{key}: {saved_state_dict[key].shape}\")\n",
    "\n",
    "# Function to load external dataset\n",
    "def load_external_dataset(path, smiles_col='Smiles', target_col='Target'):\n",
    "    df = pd.read_excel(path)\n",
    "    graphs = []\n",
    "    molecules = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        if pd.isna(row[smiles_col]):\n",
    "            continue\n",
    "            \n",
    "        g = mol_to_graph(str(row[smiles_col]))\n",
    "        if g is not None:\n",
    "            # If target is available, use it, otherwise set to None\n",
    "            if target_col in df.columns and not pd.isna(row[target_col]):\n",
    "                g.y = torch.tensor([row[target_col]], dtype=torch.float)\n",
    "            else:\n",
    "                g.y = None\n",
    "                \n",
    "            g.smiles = row[smiles_col]\n",
    "            graphs.append(g)\n",
    "            molecules.append(row.to_dict())\n",
    "    \n",
    "    return graphs, pd.DataFrame(molecules)\n",
    "\n",
    "# Custom model class that matches the saved TransformerGCN architecture\n",
    "class TransformerGCNForLoading(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers=3, \n",
    "                 num_heads=4, ff_dim_factor=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = torch.nn.Linear(in_channels, hidden_channels)\n",
    "        \n",
    "        # GCN Layers\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            self.convs.append(GCNConv(hidden_channels, hidden_channels))\n",
    "        \n",
    "        # Transformer blocks (one after each GCN layer)\n",
    "        self.transformers = torch.nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            self.transformers.append(\n",
    "                TransformerBlock(hidden_channels, num_heads, hidden_channels * ff_dim_factor, dropout)\n",
    "            )\n",
    "        \n",
    "        # Output layers\n",
    "        self.fc1 = torch.nn.Linear(hidden_channels, hidden_channels // 2)\n",
    "        self.fc2 = torch.nn.Linear(hidden_channels // 2, out_channels)\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        # Save for reshaping in forward\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, batch):\n",
    "        # Initial embedding\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # Apply GCN and Transformer layers\n",
    "        for i, (conv, transformer) in enumerate(zip(self.convs, self.transformers)):\n",
    "            # GCN Layer\n",
    "            x_conv = conv(x, edge_index)\n",
    "            x_conv = F.relu(x_conv)\n",
    "            x_conv = F.dropout(x_conv, p=self.dropout, training=self.training)\n",
    "            \n",
    "            # Reshape for transformer (using a safer approach)\n",
    "            # Create a padded tensor per batch\n",
    "            batch_idx = torch.unique(batch)\n",
    "            transformed_features = []\n",
    "            \n",
    "            for b in batch_idx:\n",
    "                # Get nodes for this batch\n",
    "                mask = (batch == b)\n",
    "                nodes_batch = x_conv[mask]\n",
    "                \n",
    "                # Process through transformer (add batch dimension)\n",
    "                if nodes_batch.size(0) > 0:  # Skip empty batches\n",
    "                    # Add batch and sequence dimensions\n",
    "                    nodes_batch = nodes_batch.unsqueeze(0)\n",
    "                    \n",
    "                    # Apply transformer\n",
    "                    transformed = transformer(nodes_batch)\n",
    "                    \n",
    "                    # Remove batch dimension\n",
    "                    transformed = transformed.squeeze(0)\n",
    "                    transformed_features.append(transformed)\n",
    "            \n",
    "            # Check if we have any valid features\n",
    "            if transformed_features:\n",
    "                # Concatenate all transformed features\n",
    "                x = torch.cat(transformed_features, dim=0)\n",
    "            else:\n",
    "                # Fallback - shouldn't typically happen\n",
    "                x = x_conv\n",
    "        \n",
    "        # Global pooling\n",
    "        x = global_mean_pool(x, batch)\n",
    "        \n",
    "        # MLP for final prediction\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Determine model parameters from saved state dict\n",
    "# We need to load the best hyperparameters from the JSON file\n",
    "try:\n",
    "    best_params = json.load(open(os.path.join(output_dir, \"study_best_params_transformer_gcn.json\"), \"r\"))\n",
    "    hidden_channels = best_params[\"hidden_channels\"]\n",
    "    num_layers = best_params[\"num_layers\"]\n",
    "    num_heads = best_params[\"num_heads\"]\n",
    "    ff_dim_factor = best_params[\"ff_dim_factor\"]\n",
    "    dropout = best_params[\"dropout\"]\n",
    "    print(f\"Loaded parameters: hidden_channels={hidden_channels}, num_layers={num_layers}, \"\n",
    "          f\"num_heads={num_heads}, ff_dim_factor={ff_dim_factor}, dropout={dropout}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading hyperparameters: {str(e)}\")\n",
    "    # Default values if JSON loading fails\n",
    "    hidden_channels = 64\n",
    "    num_layers = 3\n",
    "    num_heads = 4\n",
    "    ff_dim_factor = 4\n",
    "    dropout = 0.1\n",
    "    print(f\"Using default parameters: hidden_channels={hidden_channels}, num_layers={num_layers}, \"\n",
    "          f\"num_heads={num_heads}, ff_dim_factor={ff_dim_factor}, dropout={dropout}\")\n",
    "\n",
    "# Create the model with the correct architecture\n",
    "model = TransformerGCNForLoading(\n",
    "    in_channels=20,  # Using 20 input features as in Cell 1\n",
    "    hidden_channels=hidden_channels, \n",
    "    out_channels=1,\n",
    "    num_layers=num_layers,\n",
    "    num_heads=num_heads,\n",
    "    ff_dim_factor=ff_dim_factor,\n",
    "    dropout=dropout\n",
    ")\n",
    "\n",
    "# Try to load the saved state\n",
    "try:\n",
    "    model.load_state_dict(torch.load(os.path.join(output_dir, \"transformer_gcn_best_model.pth\")))\n",
    "    print(\"Successfully loaded model weights\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model weights: {str(e)}\")\n",
    "    print(\"This may happen if the model architecture doesn't match the saved weights.\")\n",
    "    print(\"Make sure you've trained the model with the updated architecture first.\")\n",
    "    # Continue anyway for demonstration purposes\n",
    "    print(\"Continuing with uninitialized model for demonstration...\")\n",
    "\n",
    "# Set to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Load external dataset\n",
    "external_data, external_df = load_external_dataset(\"cancer_set.xlsx\")\n",
    "# Suppress deprecation warning for DataLoader\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "    external_loader = DataLoader(external_data, batch_size=32)\n",
    "\n",
    "# Get predictions\n",
    "y_true = []\n",
    "y_probs = []\n",
    "smiles_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in external_loader:\n",
    "        try:\n",
    "            out = model(batch.x, batch.edge_index, batch.edge_attr, batch.batch)\n",
    "            # Don't squeeze - handle the dimensionality properly\n",
    "            probs = torch.sigmoid(out).cpu().numpy()\n",
    "            \n",
    "            # Handle both single-item and multi-item batches\n",
    "            if len(probs.shape) == 0:  # Single item case (0-d array)\n",
    "                probs = np.array([probs.item()])\n",
    "            \n",
    "            for i in range(len(probs)):\n",
    "                y_probs.append(probs[i][0] if probs[i].size > 1 else probs[i])\n",
    "                \n",
    "                # Get the SMILES safely\n",
    "                if hasattr(batch, 'smiles'):\n",
    "                    if isinstance(batch.smiles, list):\n",
    "                        smiles_list.append(batch.smiles[i])\n",
    "                    else:\n",
    "                        # Handle the case where smiles might be a single string\n",
    "                        smiles_list.append(batch.smiles)\n",
    "                \n",
    "                # Get the true value if it exists\n",
    "                if hasattr(batch, 'y') and batch.y is not None:\n",
    "                    if batch.y.dim() > 0 and i < batch.y.size(0):\n",
    "                        y_true.append(batch.y[i].item())\n",
    "                    elif batch.y.dim() == 0:  # Single item\n",
    "                        y_true.append(batch.y.item())\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing batch: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "# Save predictions to Excel\n",
    "results_df = pd.DataFrame({\n",
    "    'Smiles': smiles_list,\n",
    "    'Predicted_Probability': y_probs,\n",
    "    'Predicted_Class': [1 if p > 0.5 else 0 for p in y_probs]\n",
    "})\n",
    "\n",
    "# If we have true values, add them and calculate metrics\n",
    "if len(y_true) > 0 and len(y_true) == len(y_probs):\n",
    "    results_df['Actual'] = y_true\n",
    "    \n",
    "    # Calculate metrics\n",
    "    y_true_np = np.array(y_true)  # Convert list to numpy array\n",
    "    y_probs_np = np.array(y_probs)\n",
    "    y_preds = (y_probs_np > 0.5).astype(int)\n",
    "    \n",
    "    # Check if we have multiple classes for metrics that require it\n",
    "    unique_classes = np.unique(y_true_np)\n",
    "    has_multiple_classes = len(unique_classes) > 1\n",
    "    \n",
    "    metrics = {\n",
    "        'Accuracy': accuracy_score(y_true_np, y_preds),\n",
    "        'Precision': precision_score(y_true_np, y_preds, zero_division=0),\n",
    "        'F1': f1_score(y_true_np, y_preds, zero_division=0),\n",
    "        'Kappa': cohen_kappa_score(y_true_np, y_preds),\n",
    "        'Brier': brier_score_loss(y_true_np, y_probs_np),\n",
    "    }\n",
    "    \n",
    "    # Only calculate AUC and MCC if we have multiple classes\n",
    "    if has_multiple_classes:\n",
    "        # Suppress the UndefinedMetricWarning\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "            metrics['AUC'] = roc_auc_score(y_true_np, y_probs_np)\n",
    "            metrics['MCC'] = matthews_corrcoef(y_true_np, y_preds)\n",
    "    else:\n",
    "        print(\"\\n⚠️ Warning: Only one class is present in the dataset.\")\n",
    "        print(\"AUC and MCC are not defined in this case and will be reported as N/A.\")\n",
    "        metrics['AUC'] = \"N/A\"\n",
    "        metrics['MCC'] = \"N/A\"\n",
    "    \n",
    "    # Convert N/A values to NaN for Excel export\n",
    "    metrics_for_excel = {k: np.nan if v == \"N/A\" else v for k, v in metrics.items()}\n",
    "    \n",
    "    # Save metrics\n",
    "    pd.DataFrame([metrics_for_excel]).to_excel(os.path.join(output_dir, \"external_metrics_cancer.xlsx\"), index=False)\n",
    "    \n",
    "    print(\"\\n🧪 External Dataset Metrics:\")\n",
    "    for key, value in metrics.items():\n",
    "        if isinstance(value, str):\n",
    "            print(f\"{key}: {value}\")\n",
    "        else:\n",
    "            print(f\"{key}: {value:.4f}\")\n",
    "    \n",
    "    # Only generate ROC curve if we have multiple classes\n",
    "    if has_multiple_classes:\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "            fpr, tpr, _ = roc_curve(y_true_np, y_probs_np)\n",
    "            \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.plot(fpr, tpr, label=f\"AUC = {metrics['AUC']:.2f}\" if isinstance(metrics['AUC'], float) else \"AUC = N/A\")\n",
    "        plt.plot([0, 1], [0, 1], '--', color='gray')\n",
    "        plt.xlabel(\"False Positive Rate\")\n",
    "        plt.ylabel(\"True Positive Rate\")\n",
    "        plt.title(\"External Dataset ROC Curve\")\n",
    "        plt.legend()\n",
    "        plt.savefig(os.path.join(output_dir, \"external_roc_cancer.png\"))\n",
    "        plt.close()\n",
    "    \n",
    "    # Create confusion matrix\n",
    "    cm = confusion_matrix(y_true_np, y_preds)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.title('Confusion Matrix - External Dataset')\n",
    "    plt.savefig(os.path.join(output_dir, \"external_cancer_confusion_matrix.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # Class distribution information\n",
    "    class_counts = np.bincount(y_true_np.astype(int))\n",
    "    print(\"\\n📊 Class Distribution in External Dataset:\")\n",
    "    for i, count in enumerate(class_counts):\n",
    "        print(f\"Class {i}: {count} samples ({count/len(y_true_np)*100:.1f}%)\")\n",
    "\n",
    "# Save all predictions\n",
    "results_df.to_excel(os.path.join(output_dir, \"external_predictions_cancer.xlsx\"), index=False)\n",
    "print(f\"\\n💾 Predictions for {len(results_df)} molecules saved to {os.path.join(output_dir, 'external_predictions_cancer.xlsx')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e586aa64-0bce-4c6e-9e72-62bd52ed4123",
   "metadata": {},
   "source": [
    "# Explinibilty using Integrated Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f51a44c-3fab-401a-92cb-e454a3740419",
   "metadata": {},
   "source": [
    "##  Dataset used cancer_set.xlsx(FDA approved)\n",
    "- Top 40 most contributing SMILES plot\n",
    "- Feature Importance plot and saved the values in xlsx\n",
    "- Node importance vlaue saved in xlsx\n",
    "- Atom Type Importance plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591a3e96-1cb3-4502-9206-db70e455312a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.colors import Normalize\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw, AllChem\n",
    "from rdkit.Chem.Draw import rdMolDraw2D\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "import seaborn as sns\n",
    "from captum.attr import IntegratedGradients\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "import json\n",
    "\n",
    "# Create output directory\n",
    "output_dir = os.path.join('Transformer_GCN', 'external_transformer_gcn_cancer')\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Import TransformerBlock and MultiHeadAttention from original code\n",
    "class MultiHeadAttention(torch.nn.Module):\n",
    "    def __init__(self, hidden_dim, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        # Ensure hidden_dim is divisible by num_heads\n",
    "        self.head_dim = hidden_dim // num_heads\n",
    "        self.actual_dim = self.head_dim * num_heads\n",
    "        \n",
    "        self.query = torch.nn.Linear(hidden_dim, self.actual_dim)\n",
    "        self.key = torch.nn.Linear(hidden_dim, self.actual_dim)\n",
    "        self.value = torch.nn.Linear(hidden_dim, self.actual_dim)\n",
    "        self.proj = torch.nn.Linear(self.actual_dim, hidden_dim)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        seq_length = x.size(1)\n",
    "        \n",
    "        # Linear projections\n",
    "        q = self.query(x).view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k = self.key(x).view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        v = self.value(x).view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # Scaled dot-product attention\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "        \n",
    "        # Apply attention weights\n",
    "        attn_weights = torch.nn.functional.softmax(scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        context = torch.matmul(attn_weights, v)\n",
    "        \n",
    "        # Concatenate heads and put through final linear layer\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, seq_length, self.actual_dim)\n",
    "        output = self.proj(context)\n",
    "        \n",
    "        return output\n",
    "\n",
    "class TransformerBlock(torch.nn.Module):\n",
    "    def __init__(self, hidden_dim, num_heads, ff_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(hidden_dim, num_heads, dropout)\n",
    "        self.norm1 = torch.nn.LayerNorm(hidden_dim)\n",
    "        self.norm2 = torch.nn.LayerNorm(hidden_dim)\n",
    "        self.ff = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_dim, ff_dim),\n",
    "            torch.nn.GELU(),\n",
    "            torch.nn.Dropout(dropout),\n",
    "            torch.nn.Linear(ff_dim, hidden_dim),\n",
    "            torch.nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Self attention + residual connection\n",
    "        attn_output = self.attention(x)\n",
    "        x = self.norm1(x + attn_output)\n",
    "        \n",
    "        # Feed forward + residual connection\n",
    "        ff_output = self.ff(x)\n",
    "        x = self.norm2(x + ff_output)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Define the TransformerGCN model class (needed for loading the model)\n",
    "class TransformerGCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers=3, \n",
    "                 num_heads=4, ff_dim_factor=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = torch.nn.Linear(in_channels, hidden_channels)\n",
    "        \n",
    "        # GCN Layers\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            self.convs.append(GCNConv(hidden_channels, hidden_channels))\n",
    "        \n",
    "        # Transformer blocks (one after each GCN layer)\n",
    "        self.transformers = torch.nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            self.transformers.append(\n",
    "                TransformerBlock(hidden_channels, num_heads, hidden_channels * ff_dim_factor, dropout)\n",
    "            )\n",
    "        \n",
    "        # Output layers\n",
    "        self.fc1 = torch.nn.Linear(hidden_channels, hidden_channels // 2)\n",
    "        self.fc2 = torch.nn.Linear(hidden_channels // 2, out_channels)\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        # Save for reshaping in forward\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, batch):\n",
    "        # Initial embedding\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # Apply GCN and Transformer layers\n",
    "        for i, (conv, transformer) in enumerate(zip(self.convs, self.transformers)):\n",
    "            # GCN Layer\n",
    "            x_conv = conv(x, edge_index)\n",
    "            x_conv = torch.nn.functional.relu(x_conv)\n",
    "            x_conv = torch.nn.functional.dropout(x_conv, p=self.dropout, training=self.training)\n",
    "            \n",
    "            # Reshape for transformer (using a safer approach)\n",
    "            batch_idx = torch.unique(batch)\n",
    "            transformed_features = []\n",
    "            \n",
    "            for b in batch_idx:\n",
    "                # Get nodes for this batch\n",
    "                mask = (batch == b)\n",
    "                nodes_batch = x_conv[mask]\n",
    "                \n",
    "                # Process through transformer (add batch dimension)\n",
    "                if nodes_batch.size(0) > 0:  # Skip empty batches\n",
    "                    # Add batch and sequence dimensions\n",
    "                    nodes_batch = nodes_batch.unsqueeze(0)\n",
    "                    \n",
    "                    # Apply transformer\n",
    "                    transformed = transformer(nodes_batch)\n",
    "                    \n",
    "                    # Remove batch dimension\n",
    "                    transformed = transformed.squeeze(0)\n",
    "                    transformed_features.append(transformed)\n",
    "            \n",
    "            # Check if we have any valid features\n",
    "            if transformed_features:\n",
    "                # Concatenate all transformed features\n",
    "                x = torch.cat(transformed_features, dim=0)\n",
    "            else:\n",
    "                # Fallback - shouldn't typically happen\n",
    "                x = x_conv\n",
    "        \n",
    "        # Global pooling\n",
    "        x = global_mean_pool(x, batch)\n",
    "        \n",
    "        # MLP for final prediction\n",
    "        x = self.fc1(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = torch.nn.functional.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Function to convert molecules to graph data\n",
    "def atom_features(atom):\n",
    "    return torch.tensor([\n",
    "        # Basic properties\n",
    "        atom.GetAtomicNum(),                     # Atomic number\n",
    "        atom.GetDegree(),                        # Number of bonded neighbors\n",
    "        atom.GetFormalCharge(),                  # Formal charge\n",
    "        atom.GetNumRadicalElectrons(),           # Number of radical electrons\n",
    "        int(atom.GetIsAromatic()),               # Aromaticity flag\n",
    "        \n",
    "        # Extended properties\n",
    "        atom.GetExplicitValence(),               # Explicit valence\n",
    "        atom.GetImplicitValence(),               # Implicit valence\n",
    "        atom.GetTotalValence(),                  # Total valence\n",
    "        atom.GetNumImplicitHs(),                 # Number of implicit hydrogens\n",
    "        atom.GetHybridization(),                 # Hybridization state\n",
    "        atom.GetTotalNumHs(),                    # Total number of hydrogens\n",
    "        \n",
    "        # Topological properties\n",
    "        int(atom.IsInRing()),                    # Whether the atom is in a ring\n",
    "        int(atom.IsInRingSize(3)),               # Whether in 3-membered ring\n",
    "        int(atom.IsInRingSize(4)),               # Whether in 4-membered ring\n",
    "        int(atom.IsInRingSize(5)),               # Whether in 5-membered ring\n",
    "        int(atom.IsInRingSize(6)),               # Whether in 6-membered ring\n",
    "        int(atom.IsInRingSize(7)),               # Whether in 7-membered ring\n",
    "        \n",
    "        # Electronic properties\n",
    "        atom.GetChiralTag(),                     # Chirality\n",
    "        atom.GetMass(),                          # Atomic mass\n",
    "        Chem.rdMolDescriptors.CalcCrippenDescriptors(\n",
    "            Chem.MolFromSmiles(f\"[{atom.GetSymbol()}]\"))[0]  # LogP contribution\n",
    "    ], dtype=torch.float)\n",
    "\n",
    "def mol_to_graph(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None: return None\n",
    "    atoms = [atom_features(atom) for atom in mol.GetAtoms()]\n",
    "    if not atoms: return None\n",
    "    x = torch.stack(atoms, dim=0)\n",
    "    edge_index = []\n",
    "    for bond in mol.GetBonds():\n",
    "        i, j = bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()\n",
    "        edge_index += [[i, j], [j, i]]\n",
    "    edge_index = torch.tensor(edge_index).t().contiguous()\n",
    "    edge_attr = [[bond.GetBondTypeAsDouble(), bond.GetIsConjugated(), bond.IsInRing()] * 2 for bond in mol.GetBonds()]\n",
    "    edge_attr = torch.tensor(edge_attr, dtype=torch.float).reshape(-1, 3)\n",
    "    from torch_geometric.data import Data\n",
    "    return Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "\n",
    "def load_dataset(path, smiles_col='Smiles', target_col='Target'):\n",
    "    df = pd.read_excel(path)\n",
    "    graphs = []\n",
    "    for _, row in df.iterrows():\n",
    "        if pd.isna(row[smiles_col]) or pd.isna(row[target_col]):\n",
    "            continue\n",
    "        g = mol_to_graph(str(row[smiles_col]))\n",
    "        if g is not None:\n",
    "            g.y = torch.tensor([row[target_col]], dtype=torch.float)\n",
    "            g.smiles = row[smiles_col]\n",
    "            graphs.append(g)\n",
    "    return graphs\n",
    "\n",
    "# Load the trained model\n",
    "def load_trained_model(model_path, input_dim=20, hidden_params_path=os.path.join('Transformer_GCN', 'study_best_params_transformer_gcn.json')):\n",
    "    try:\n",
    "        with open(hidden_params_path, 'r') as f:\n",
    "            best_params = json.load(f)\n",
    "        \n",
    "        model = TransformerGCN(\n",
    "            in_channels=input_dim,\n",
    "            hidden_channels=best_params['hidden_channels'],\n",
    "            out_channels=1, \n",
    "            num_layers=best_params['num_layers'], \n",
    "            num_heads=best_params['num_heads'],\n",
    "            ff_dim_factor=best_params['ff_dim_factor'], \n",
    "            dropout=best_params['dropout']\n",
    "        )\n",
    "    except FileNotFoundError:\n",
    "        # Fallback to default parameters if file not found\n",
    "        print(f\"Parameters file not found at {hidden_params_path}. Using default parameters.\")\n",
    "        model = TransformerGCN(\n",
    "            in_channels=input_dim,\n",
    "            hidden_channels=64,\n",
    "            out_channels=1,\n",
    "            num_layers=3,\n",
    "            num_heads=4,\n",
    "            ff_dim_factor=4,\n",
    "            dropout=0.1\n",
    "        )\n",
    "    \n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# Load model\n",
    "model_path = os.path.join('Transformer_GCN', 'transformer_gcn_best_model.pth')\n",
    "model = load_trained_model(model_path)\n",
    "\n",
    "# Load dataset\n",
    "train_data = load_dataset(\"cancer_set.xlsx\")  # Load your full dataset\n",
    "\n",
    "# Helper function to create a wrapper model for attribution methods\n",
    "class TransformerGCNWrapper(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        \n",
    "    def forward(self, x, edge_index, edge_attr, batch):\n",
    "        return torch.sigmoid(self.model(x, edge_index, edge_attr, batch))\n",
    "\n",
    "# Initialize attribution method\n",
    "wrapper_model = TransformerGCNWrapper(model)\n",
    "integrated_gradients = IntegratedGradients(wrapper_model)\n",
    "\n",
    "# Function to interpret molecule graph and get node/edge importance\n",
    "def interpret_molecule(graph):\n",
    "    graph_batch = graph.clone()\n",
    "    graph_batch.batch = torch.zeros(graph.x.shape[0], dtype=torch.long)\n",
    "    \n",
    "    # Get integrated gradients attribution\n",
    "    node_attr = integrated_gradients.attribute(\n",
    "        graph.x, \n",
    "        additional_forward_args=(graph.edge_index, graph.edge_attr, graph_batch.batch),\n",
    "        internal_batch_size=1\n",
    "    )\n",
    "    \n",
    "    # Sum across feature dimensions to get per-node importance\n",
    "    node_importance = torch.abs(node_attr).sum(dim=1).detach().numpy()\n",
    "    \n",
    "    # For edge importance, we'll use a simple approximation based on connected nodes\n",
    "    edge_importance = []\n",
    "    for i in range(graph.edge_index.shape[1]):\n",
    "        src, dst = graph.edge_index[0, i].item(), graph.edge_index[1, i].item()\n",
    "        edge_imp = (node_importance[src] + node_importance[dst]) / 2\n",
    "        edge_importance.append(edge_imp)\n",
    "    \n",
    "    return node_importance, np.array(edge_importance)\n",
    "\n",
    "# Function to create a safe filename from SMILES\n",
    "def create_safe_filename(smiles):\n",
    "    filename = f\"{smiles}_transformer_gcn.png\"\n",
    "    # Replace characters that are problematic in filenames\n",
    "    for char in ['/', '\\\\', ':', '*', '?', '\"', '<', '>', '|']:\n",
    "        filename = filename.replace(char, '_')\n",
    "    # Ensure the filename isn't too long for the filesystem\n",
    "    if len(filename) > 200:\n",
    "        # Truncate but keep the suffix\n",
    "        filename = filename[:185] + \"_tgcn_cancer.png\"\n",
    "    return filename\n",
    "\n",
    "# Function to visualize molecule with importance highlighting\n",
    "def visualize_molecule_importance(smiles, node_importance, mol=None, title=None, size=(600, 600)):\n",
    "    if mol is None:\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "    \n",
    "    # Normalize importances for coloring\n",
    "    if len(node_importance) > 0:\n",
    "        norm = Normalize(vmin=np.min(node_importance), vmax=np.max(node_importance))\n",
    "        atom_colors = {}\n",
    "        for i, imp in enumerate(node_importance):\n",
    "            color_val = norm(imp)\n",
    "            atom_colors[i] = tuple(plt.cm.viridis(color_val)[:3])\n",
    "    else:\n",
    "        atom_colors = {}\n",
    "    \n",
    "    # Create drawing\n",
    "    drawer = rdMolDraw2D.MolDraw2DCairo(size[0], size[1])\n",
    "    drawer.SetFontSize(0.8)\n",
    "    \n",
    "    # Set up atom highlights\n",
    "    highlight_atoms = list(range(mol.GetNumAtoms()))\n",
    "    highlight_atom_colors = atom_colors\n",
    "    \n",
    "    # Add title if provided\n",
    "    if title:\n",
    "        drawer.DrawMoleculeWithHighlights(\n",
    "            mol, title, \n",
    "            highlightAtoms=highlight_atoms,\n",
    "            highlightAtomColors=highlight_atom_colors\n",
    "        )\n",
    "    else:\n",
    "        # Draw the molecule with highlights\n",
    "        drawer.DrawMolecule(\n",
    "            mol,\n",
    "            highlightAtoms=highlight_atoms,\n",
    "            highlightAtomColors=highlight_atom_colors\n",
    "        )\n",
    "    drawer.FinishDrawing()\n",
    "    \n",
    "    return drawer.GetDrawingText()\n",
    "\n",
    "# Function to analyze feature importance across the dataset\n",
    "def analyze_feature_importance(data_list, top_n=20):\n",
    "    all_attributions = []\n",
    "    \n",
    "    for graph in tqdm(data_list, desc=\"Analyzing feature importance\"):\n",
    "        graph_batch = graph.clone()\n",
    "        graph_batch.batch = torch.zeros(graph.x.shape[0], dtype=torch.long)\n",
    "        \n",
    "        attr = integrated_gradients.attribute(\n",
    "            graph.x, \n",
    "            additional_forward_args=(graph.edge_index, graph.edge_attr, graph_batch.batch),\n",
    "            internal_batch_size=1\n",
    "        )\n",
    "        \n",
    "        # Average attribution per feature across all nodes\n",
    "        avg_attr = torch.abs(attr).mean(dim=0).detach().numpy()\n",
    "        all_attributions.append(avg_attr)\n",
    "    \n",
    "    # Average across all molecules\n",
    "    feature_importance = np.mean(all_attributions, axis=0)\n",
    "    \n",
    "    # Feature names (same as in atom_features function)\n",
    "    feature_names = [\n",
    "        'Atomic number', 'Degree', 'Formal charge', 'Radical electrons', 'Is aromatic',\n",
    "        'Explicit valence', 'Implicit valence', 'Total valence', 'Implicit Hs',\n",
    "        'Hybridization', 'Total Hs', 'In ring', 'In ring size 3', 'In ring size 4',\n",
    "        'In ring size 5', 'In ring size 6', 'In ring size 7', 'Chiral tag', 'Mass',\n",
    "        'LogP contribution'\n",
    "    ]\n",
    "    \n",
    "    # Sort by importance\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': feature_importance\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    return importance_df\n",
    "\n",
    "# Create output subfolders\n",
    "viz_dir = os.path.join(output_dir, 'molecule_viz_cancer')\n",
    "os.makedirs(viz_dir, exist_ok=True)\n",
    "\n",
    "top_molecules_dir = os.path.join(output_dir, 'top_molecules')\n",
    "os.makedirs(top_molecules_dir, exist_ok=True)\n",
    "\n",
    "# Process all molecules in the dataset\n",
    "molecule_importances = []\n",
    "\n",
    "# Process a subset of molecules (limit to 100 for efficiency)\n",
    "num_molecules = min(100, len(train_data))\n",
    "\n",
    "for i, graph in enumerate(tqdm(train_data[:num_molecules], desc=\"Processing molecules\")):\n",
    "    try:\n",
    "        smiles = graph.smiles\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is None:\n",
    "            continue\n",
    "            \n",
    "        # Get prediction score\n",
    "        graph_batch = graph.clone()\n",
    "        graph_batch.batch = torch.zeros(graph.x.shape[0], dtype=torch.long)\n",
    "        pred_score = wrapper_model(graph.x, graph.edge_index, graph.edge_attr, graph_batch.batch).item()\n",
    "        \n",
    "        # Get node and edge importance\n",
    "        node_importance, edge_importance = interpret_molecule(graph)\n",
    "        \n",
    "        # Calculate importance metrics\n",
    "        avg_importance = np.mean(node_importance)\n",
    "        max_importance = np.max(node_importance)\n",
    "        \n",
    "        # Create a safe filename from the SMILES\n",
    "        safe_filename = create_safe_filename(smiles)\n",
    "        \n",
    "        # Save molecule image with importance visualization\n",
    "        img_data = visualize_molecule_importance(smiles, node_importance, mol)\n",
    "        with open(os.path.join(viz_dir, safe_filename), \"wb\") as f:\n",
    "            f.write(img_data)\n",
    "        \n",
    "        # Add to our results dataframe\n",
    "        molecule_importances.append({\n",
    "            'Index': i+1,\n",
    "            'SMILES': smiles,\n",
    "            'Prediction': pred_score,\n",
    "            'True_Label': graph.y.item(),\n",
    "            'Average_Importance': avg_importance,\n",
    "            'Max_Importance': max_importance,\n",
    "            'Most_Important_Atom_Index': np.argmax(node_importance),\n",
    "            'Filename': safe_filename\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing molecule {i+1}: {e}\")\n",
    "\n",
    "# Create and save dataframe of molecule importances\n",
    "importance_df = pd.DataFrame(molecule_importances)\n",
    "importance_df.to_excel(os.path.join(output_dir, 'molecule_importances_cancer.xlsx'), index=False)\n",
    "\n",
    "# Create feature importance analysis\n",
    "feature_imp_df = analyze_feature_importance(train_data[:num_molecules])\n",
    "feature_imp_df.to_excel(os.path.join(output_dir, 'feature_importance_cancer.xlsx'), index=False)\n",
    "\n",
    "# Plot and save feature importance graph\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Importance', y='Feature', data=feature_imp_df.head(15))\n",
    "plt.title('Top 15 Important Atom Features (TransformerGCN Analysis)')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, 'feature_importance_cancer.png'), dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# Find the top 40 molecules by importance\n",
    "top_molecules_count = min(40, len(importance_df))\n",
    "top_mols = importance_df.sort_values('Average_Importance', ascending=False).head(top_molecules_count)\n",
    "\n",
    "# Create a top SMILES CSV file\n",
    "with open(os.path.join(output_dir, \"top_smiles_cancer.csv\"), \"w\") as f:\n",
    "    f.write(\"Rank,MoleculeIndex,SMILES,Prediction,TrueLabel,AverageImportance,Filename\\n\")\n",
    "    for j, (_, r) in enumerate(top_mols.iterrows()):\n",
    "        f.write(f\"{j+1},{r['Index']},{r['SMILES']},{r['Prediction']:.4f},{r['True_Label']:.0f},{r['Average_Importance']:.4f},{r['Filename']}\\n\")\n",
    "\n",
    "# Create a higher-quality version of the top molecules (with titles)\n",
    "for i, (_, row) in enumerate(tqdm(top_mols.iterrows(), desc=\"Creating high-quality top molecule images\")):\n",
    "    try:\n",
    "        # Get the molecule and its data\n",
    "        idx = row['Index']\n",
    "        smiles = row['SMILES']\n",
    "        pred = row['Prediction']\n",
    "        true_label = row['True_Label']\n",
    "        \n",
    "        # Get the corresponding graph data\n",
    "        graph = train_data[idx-1]  # -1 because indices start at 1 in our data\n",
    "        \n",
    "        # Re-interpret the molecule for visualization\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is None:\n",
    "            continue\n",
    "            \n",
    "        # Get node importance\n",
    "        node_importance, _ = interpret_molecule(graph)\n",
    "        \n",
    "        # Create a title with info\n",
    "        title = f\"#{idx}: Pred={pred:.3f}, True={true_label:.0f}, Importance={row['Average_Importance']:.3f}\"\n",
    "        \n",
    "        # Create a high-quality visualization\n",
    "        img_data = visualize_molecule_importance(smiles, node_importance, mol, title=title, size=(800, 800))\n",
    "        \n",
    "        # Use the same filename as before\n",
    "        safe_filename = row['Filename']\n",
    "        \n",
    "        # Save to the top molecules directory\n",
    "        with open(os.path.join(top_molecules_dir, safe_filename), \"wb\") as f:\n",
    "            f.write(img_data)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating top molecule image for rank {i+1}: {e}\")\n",
    "\n",
    "# Create node importance analysis\n",
    "node_importances = []\n",
    "for i, graph in enumerate(tqdm(train_data[:num_molecules], desc=\"Analyzing node types\")):\n",
    "    try:\n",
    "        smiles = graph.smiles\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is None:\n",
    "            continue\n",
    "            \n",
    "        node_importance, _ = interpret_molecule(graph)\n",
    "        \n",
    "        # Get atom types and their importance\n",
    "        for atom_idx, importance in enumerate(node_importance):\n",
    "            atom_symbol = mol.GetAtomWithIdx(atom_idx).GetSymbol()\n",
    "            node_importances.append({\n",
    "                'Molecule_Index': i+1,\n",
    "                'SMILES': smiles,\n",
    "                'Atom_Index': atom_idx,\n",
    "                'Atom_Symbol': atom_symbol,\n",
    "                'Importance': importance\n",
    "            })\n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing node for molecule {i+1}: {e}\")\n",
    "\n",
    "# Create and save node importance dataframe\n",
    "node_imp_df = pd.DataFrame(node_importances)\n",
    "node_imp_df.to_excel(os.path.join(output_dir, 'node_importances_cancer.xlsx'), index=False)\n",
    "\n",
    "# Analyze importance by atom type\n",
    "atom_importance = node_imp_df.groupby('Atom_Symbol')['Importance'].mean().reset_index()\n",
    "atom_importance = atom_importance.sort_values('Importance', ascending=False)\n",
    "atom_importance.to_excel(os.path.join(output_dir, 'atom_type_importance_cancer.xlsx'), index=False)\n",
    "\n",
    "# Plot atom type importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Importance', y='Atom_Symbol', data=atom_importance)\n",
    "plt.title('Average Importance by Atom Type (TransformerGCN Analysis)')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, 'atom_type_importance_cancer.png'), dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# Create a color-coded Excel file with molecule importances\n",
    "try:\n",
    "    from openpyxl import load_workbook\n",
    "    from openpyxl.styles import PatternFill\n",
    "    from openpyxl.styles.differential import DifferentialStyle\n",
    "    from openpyxl.formatting.rule import ColorScaleRule\n",
    "    \n",
    "    color_excel_path = os.path.join(output_dir, 'molecule_importances_colored_cancer.xlsx')\n",
    "    importance_df.to_excel(color_excel_path, index=False)\n",
    "    \n",
    "    # Open the workbook and add color scales\n",
    "    wb = load_workbook(color_excel_path)\n",
    "    ws = wb.active\n",
    "    \n",
    "    # Add color scale to Average_Importance column\n",
    "    col_idx = importance_df.columns.get_loc(\"Average_Importance\") + 1  # +1 because Excel is 1-indexed\n",
    "    col_letter = ws.cell(1, col_idx).column_letter\n",
    "    \n",
    "    ws.conditional_formatting.add(\n",
    "        f\"{col_letter}2:{col_letter}{len(importance_df)+1}\",\n",
    "        ColorScaleRule(\n",
    "            start_type='min', start_color='FFFFFF',\n",
    "            mid_type='percentile', mid_value=50, mid_color='FFFF00',\n",
    "            end_type='max', end_color='FF0000'\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Add color scale to Max_Importance column\n",
    "    col_idx = importance_df.columns.get_loc(\"Max_Importance\") + 1\n",
    "    col_letter = ws.cell(1, col_idx).column_letter\n",
    "    \n",
    "    ws.conditional_formatting.add(\n",
    "        f\"{col_letter}2:{col_letter}{len(importance_df)+1}\",\n",
    "        ColorScaleRule(\n",
    "            start_type='min', start_color='FFFFFF',\n",
    "            mid_type='percentile', mid_value=50, mid_color='FFFF00',\n",
    "            end_type='max', end_color='FF0000'\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Save the workbook\n",
    "    wb.save(color_excel_path)\n",
    "    print(f\"Created color-coded Excel file: {color_excel_path}\")\n",
    "except ImportError:\n",
    "    print(\"openpyxl not available - skipping colored Excel creation\")\n",
    "\n",
    "print(f\"✅ Analysis complete! All results saved to {output_dir}\")\n",
    "print(f\"📊 Key files generated:\")\n",
    "print(f\" - Molecule visualizations with SMILES filenames: {viz_dir}/*.png\")\n",
    "print(f\" - Top 40 molecules: {top_molecules_dir}/*.png\")\n",
    "print(f\" - Top molecules SMILES list: {os.path.join(output_dir, 'top_smiles_cancer.csv')}\")\n",
    "print(f\" - Feature importance analysis: {os.path.join(output_dir, 'feature_importance_cancer.xlsx')}\")\n",
    "print(f\" - Molecule importance spreadsheet: {os.path.join(output_dir, 'molecule_importances_cancer.xlsx')}\")\n",
    "print(f\" - Atom type importance: {os.path.join(output_dir, 'atom_type_importance_cancer.png')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7448e7-d433-4ef7-9426-3734714047f0",
   "metadata": {},
   "source": [
    "##  Dataset used dataset_main.xlsx\n",
    "- Top 40 most contributing SMILES plot\n",
    "- Feature Importance plot and saved the values in xlsx\n",
    "- Node importance vlaue saved in xlsx\n",
    "- Atom Type Importance plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736f9e69-3931-4b32-a05b-e2251cbea206",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.colors import Normalize\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw, AllChem\n",
    "from rdkit.Chem.Draw import rdMolDraw2D\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "import seaborn as sns\n",
    "from captum.attr import IntegratedGradients\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "import json\n",
    "\n",
    "# Create output directory\n",
    "output_dir = os.path.join('Transformer_GCN', 'external_transformer_gcn')\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Import TransformerBlock and MultiHeadAttention from original code\n",
    "class MultiHeadAttention(torch.nn.Module):\n",
    "    def __init__(self, hidden_dim, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        # Ensure hidden_dim is divisible by num_heads\n",
    "        self.head_dim = hidden_dim // num_heads\n",
    "        self.actual_dim = self.head_dim * num_heads\n",
    "        \n",
    "        self.query = torch.nn.Linear(hidden_dim, self.actual_dim)\n",
    "        self.key = torch.nn.Linear(hidden_dim, self.actual_dim)\n",
    "        self.value = torch.nn.Linear(hidden_dim, self.actual_dim)\n",
    "        self.proj = torch.nn.Linear(self.actual_dim, hidden_dim)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        seq_length = x.size(1)\n",
    "        \n",
    "        # Linear projections\n",
    "        q = self.query(x).view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k = self.key(x).view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        v = self.value(x).view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # Scaled dot-product attention\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "        \n",
    "        # Apply attention weights\n",
    "        attn_weights = torch.nn.functional.softmax(scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        context = torch.matmul(attn_weights, v)\n",
    "        \n",
    "        # Concatenate heads and put through final linear layer\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, seq_length, self.actual_dim)\n",
    "        output = self.proj(context)\n",
    "        \n",
    "        return output\n",
    "\n",
    "class TransformerBlock(torch.nn.Module):\n",
    "    def __init__(self, hidden_dim, num_heads, ff_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(hidden_dim, num_heads, dropout)\n",
    "        self.norm1 = torch.nn.LayerNorm(hidden_dim)\n",
    "        self.norm2 = torch.nn.LayerNorm(hidden_dim)\n",
    "        self.ff = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_dim, ff_dim),\n",
    "            torch.nn.GELU(),\n",
    "            torch.nn.Dropout(dropout),\n",
    "            torch.nn.Linear(ff_dim, hidden_dim),\n",
    "            torch.nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Self attention + residual connection\n",
    "        attn_output = self.attention(x)\n",
    "        x = self.norm1(x + attn_output)\n",
    "        \n",
    "        # Feed forward + residual connection\n",
    "        ff_output = self.ff(x)\n",
    "        x = self.norm2(x + ff_output)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Define the TransformerGCN model class (needed for loading the model)\n",
    "class TransformerGCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers=3, \n",
    "                 num_heads=4, ff_dim_factor=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = torch.nn.Linear(in_channels, hidden_channels)\n",
    "        \n",
    "        # GCN Layers\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            self.convs.append(GCNConv(hidden_channels, hidden_channels))\n",
    "        \n",
    "        # Transformer blocks (one after each GCN layer)\n",
    "        self.transformers = torch.nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            self.transformers.append(\n",
    "                TransformerBlock(hidden_channels, num_heads, hidden_channels * ff_dim_factor, dropout)\n",
    "            )\n",
    "        \n",
    "        # Output layers\n",
    "        self.fc1 = torch.nn.Linear(hidden_channels, hidden_channels // 2)\n",
    "        self.fc2 = torch.nn.Linear(hidden_channels // 2, out_channels)\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        # Save for reshaping in forward\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, batch):\n",
    "        # Initial embedding\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # Apply GCN and Transformer layers\n",
    "        for i, (conv, transformer) in enumerate(zip(self.convs, self.transformers)):\n",
    "            # GCN Layer\n",
    "            x_conv = conv(x, edge_index)\n",
    "            x_conv = torch.nn.functional.relu(x_conv)\n",
    "            x_conv = torch.nn.functional.dropout(x_conv, p=self.dropout, training=self.training)\n",
    "            \n",
    "            # Reshape for transformer (using a safer approach)\n",
    "            batch_idx = torch.unique(batch)\n",
    "            transformed_features = []\n",
    "            \n",
    "            for b in batch_idx:\n",
    "                # Get nodes for this batch\n",
    "                mask = (batch == b)\n",
    "                nodes_batch = x_conv[mask]\n",
    "                \n",
    "                # Process through transformer (add batch dimension)\n",
    "                if nodes_batch.size(0) > 0:  # Skip empty batches\n",
    "                    # Add batch and sequence dimensions\n",
    "                    nodes_batch = nodes_batch.unsqueeze(0)\n",
    "                    \n",
    "                    # Apply transformer\n",
    "                    transformed = transformer(nodes_batch)\n",
    "                    \n",
    "                    # Remove batch dimension\n",
    "                    transformed = transformed.squeeze(0)\n",
    "                    transformed_features.append(transformed)\n",
    "            \n",
    "            # Check if we have any valid features\n",
    "            if transformed_features:\n",
    "                # Concatenate all transformed features\n",
    "                x = torch.cat(transformed_features, dim=0)\n",
    "            else:\n",
    "                # Fallback - shouldn't typically happen\n",
    "                x = x_conv\n",
    "        \n",
    "        # Global pooling\n",
    "        x = global_mean_pool(x, batch)\n",
    "        \n",
    "        # MLP for final prediction\n",
    "        x = self.fc1(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = torch.nn.functional.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Function to convert molecules to graph data\n",
    "def atom_features(atom):\n",
    "    return torch.tensor([\n",
    "        # Basic properties\n",
    "        atom.GetAtomicNum(),                     # Atomic number\n",
    "        atom.GetDegree(),                        # Number of bonded neighbors\n",
    "        atom.GetFormalCharge(),                  # Formal charge\n",
    "        atom.GetNumRadicalElectrons(),           # Number of radical electrons\n",
    "        int(atom.GetIsAromatic()),               # Aromaticity flag\n",
    "        \n",
    "        # Extended properties\n",
    "        atom.GetExplicitValence(),               # Explicit valence\n",
    "        atom.GetImplicitValence(),               # Implicit valence\n",
    "        atom.GetTotalValence(),                  # Total valence\n",
    "        atom.GetNumImplicitHs(),                 # Number of implicit hydrogens\n",
    "        atom.GetHybridization(),                 # Hybridization state\n",
    "        atom.GetTotalNumHs(),                    # Total number of hydrogens\n",
    "        \n",
    "        # Topological properties\n",
    "        int(atom.IsInRing()),                    # Whether the atom is in a ring\n",
    "        int(atom.IsInRingSize(3)),               # Whether in 3-membered ring\n",
    "        int(atom.IsInRingSize(4)),               # Whether in 4-membered ring\n",
    "        int(atom.IsInRingSize(5)),               # Whether in 5-membered ring\n",
    "        int(atom.IsInRingSize(6)),               # Whether in 6-membered ring\n",
    "        int(atom.IsInRingSize(7)),               # Whether in 7-membered ring\n",
    "        \n",
    "        # Electronic properties\n",
    "        atom.GetChiralTag(),                     # Chirality\n",
    "        atom.GetMass(),                          # Atomic mass\n",
    "        Chem.rdMolDescriptors.CalcCrippenDescriptors(\n",
    "            Chem.MolFromSmiles(f\"[{atom.GetSymbol()}]\"))[0]  # LogP contribution\n",
    "    ], dtype=torch.float)\n",
    "\n",
    "def mol_to_graph(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None: return None\n",
    "    atoms = [atom_features(atom) for atom in mol.GetAtoms()]\n",
    "    if not atoms: return None\n",
    "    x = torch.stack(atoms, dim=0)\n",
    "    edge_index = []\n",
    "    for bond in mol.GetBonds():\n",
    "        i, j = bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()\n",
    "        edge_index += [[i, j], [j, i]]\n",
    "    edge_index = torch.tensor(edge_index).t().contiguous()\n",
    "    edge_attr = [[bond.GetBondTypeAsDouble(), bond.GetIsConjugated(), bond.IsInRing()] * 2 for bond in mol.GetBonds()]\n",
    "    edge_attr = torch.tensor(edge_attr, dtype=torch.float).reshape(-1, 3)\n",
    "    from torch_geometric.data import Data\n",
    "    return Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "\n",
    "def load_dataset(path, smiles_col='Smiles', target_col='Target'):\n",
    "    df = pd.read_excel(path)\n",
    "    graphs = []\n",
    "    for _, row in df.iterrows():\n",
    "        if pd.isna(row[smiles_col]) or pd.isna(row[target_col]):\n",
    "            continue\n",
    "        g = mol_to_graph(str(row[smiles_col]))\n",
    "        if g is not None:\n",
    "            g.y = torch.tensor([row[target_col]], dtype=torch.float)\n",
    "            g.smiles = row[smiles_col]\n",
    "            graphs.append(g)\n",
    "    return graphs\n",
    "\n",
    "# Load the trained model\n",
    "def load_trained_model(model_path, input_dim=20, hidden_params_path=os.path.join('Transformer_GCN', 'study_best_params_transformer_gcn.json')):\n",
    "    try:\n",
    "        with open(hidden_params_path, 'r') as f:\n",
    "            best_params = json.load(f)\n",
    "        \n",
    "        model = TransformerGCN(\n",
    "            in_channels=input_dim,\n",
    "            hidden_channels=best_params['hidden_channels'],\n",
    "            out_channels=1, \n",
    "            num_layers=best_params['num_layers'], \n",
    "            num_heads=best_params['num_heads'],\n",
    "            ff_dim_factor=best_params['ff_dim_factor'], \n",
    "            dropout=best_params['dropout']\n",
    "        )\n",
    "    except FileNotFoundError:\n",
    "        # Fallback to default parameters if file not found\n",
    "        print(f\"Parameters file not found at {hidden_params_path}. Using default parameters.\")\n",
    "        model = TransformerGCN(\n",
    "            in_channels=input_dim,\n",
    "            hidden_channels=64,\n",
    "            out_channels=1,\n",
    "            num_layers=3,\n",
    "            num_heads=4,\n",
    "            ff_dim_factor=4,\n",
    "            dropout=0.1\n",
    "        )\n",
    "    \n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# Load model\n",
    "model_path = os.path.join('Transformer_GCN', 'transformer_gcn_best_model.pth')\n",
    "model = load_trained_model(model_path)\n",
    "\n",
    "# Load dataset\n",
    "train_data = load_dataset(\"dataset_main.xlsx\")  # Load your full dataset\n",
    "\n",
    "# Helper function to create a wrapper model for attribution methods\n",
    "class TransformerGCNWrapper(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        \n",
    "    def forward(self, x, edge_index, edge_attr, batch):\n",
    "        return torch.sigmoid(self.model(x, edge_index, edge_attr, batch))\n",
    "\n",
    "# Initialize attribution method\n",
    "wrapper_model = TransformerGCNWrapper(model)\n",
    "integrated_gradients = IntegratedGradients(wrapper_model)\n",
    "\n",
    "# Function to interpret molecule graph and get node/edge importance\n",
    "def interpret_molecule(graph):\n",
    "    graph_batch = graph.clone()\n",
    "    graph_batch.batch = torch.zeros(graph.x.shape[0], dtype=torch.long)\n",
    "    \n",
    "    # Get integrated gradients attribution\n",
    "    node_attr = integrated_gradients.attribute(\n",
    "        graph.x, \n",
    "        additional_forward_args=(graph.edge_index, graph.edge_attr, graph_batch.batch),\n",
    "        internal_batch_size=1\n",
    "    )\n",
    "    \n",
    "    # Sum across feature dimensions to get per-node importance\n",
    "    node_importance = torch.abs(node_attr).sum(dim=1).detach().numpy()\n",
    "    \n",
    "    # For edge importance, we'll use a simple approximation based on connected nodes\n",
    "    edge_importance = []\n",
    "    for i in range(graph.edge_index.shape[1]):\n",
    "        src, dst = graph.edge_index[0, i].item(), graph.edge_index[1, i].item()\n",
    "        edge_imp = (node_importance[src] + node_importance[dst]) / 2\n",
    "        edge_importance.append(edge_imp)\n",
    "    \n",
    "    return node_importance, np.array(edge_importance)\n",
    "\n",
    "# Function to create a safe filename from SMILES\n",
    "def create_safe_filename(smiles):\n",
    "    filename = f\"{smiles}_transformer_gcn.png\"\n",
    "    # Replace characters that are problematic in filenames\n",
    "    for char in ['/', '\\\\', ':', '*', '?', '\"', '<', '>', '|']:\n",
    "        filename = filename.replace(char, '_')\n",
    "    # Ensure the filename isn't too long for the filesystem\n",
    "    if len(filename) > 200:\n",
    "        # Truncate but keep the suffix\n",
    "        filename = filename[:185] + \"_tgcn.png\"\n",
    "    return filename\n",
    "\n",
    "# Function to visualize molecule with importance highlighting\n",
    "def visualize_molecule_importance(smiles, node_importance, mol=None, title=None, size=(600, 600)):\n",
    "    if mol is None:\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "    \n",
    "    # Normalize importances for coloring\n",
    "    if len(node_importance) > 0:\n",
    "        norm = Normalize(vmin=np.min(node_importance), vmax=np.max(node_importance))\n",
    "        atom_colors = {}\n",
    "        for i, imp in enumerate(node_importance):\n",
    "            color_val = norm(imp)\n",
    "            atom_colors[i] = tuple(plt.cm.viridis(color_val)[:3])\n",
    "    else:\n",
    "        atom_colors = {}\n",
    "    \n",
    "    # Create drawing\n",
    "    drawer = rdMolDraw2D.MolDraw2DCairo(size[0], size[1])\n",
    "    drawer.SetFontSize(0.8)\n",
    "    \n",
    "    # Set up atom highlights\n",
    "    highlight_atoms = list(range(mol.GetNumAtoms()))\n",
    "    highlight_atom_colors = atom_colors\n",
    "    \n",
    "    # Add title if provided\n",
    "    if title:\n",
    "        drawer.DrawMoleculeWithHighlights(\n",
    "            mol, title, \n",
    "            highlightAtoms=highlight_atoms,\n",
    "            highlightAtomColors=highlight_atom_colors\n",
    "        )\n",
    "    else:\n",
    "        # Draw the molecule with highlights\n",
    "        drawer.DrawMolecule(\n",
    "            mol,\n",
    "            highlightAtoms=highlight_atoms,\n",
    "            highlightAtomColors=highlight_atom_colors\n",
    "        )\n",
    "    drawer.FinishDrawing()\n",
    "    \n",
    "    return drawer.GetDrawingText()\n",
    "\n",
    "# Function to analyze feature importance across the dataset\n",
    "def analyze_feature_importance(data_list, top_n=20):\n",
    "    all_attributions = []\n",
    "    \n",
    "    for graph in tqdm(data_list, desc=\"Analyzing feature importance\"):\n",
    "        graph_batch = graph.clone()\n",
    "        graph_batch.batch = torch.zeros(graph.x.shape[0], dtype=torch.long)\n",
    "        \n",
    "        attr = integrated_gradients.attribute(\n",
    "            graph.x, \n",
    "            additional_forward_args=(graph.edge_index, graph.edge_attr, graph_batch.batch),\n",
    "            internal_batch_size=1\n",
    "        )\n",
    "        \n",
    "        # Average attribution per feature across all nodes\n",
    "        avg_attr = torch.abs(attr).mean(dim=0).detach().numpy()\n",
    "        all_attributions.append(avg_attr)\n",
    "    \n",
    "    # Average across all molecules\n",
    "    feature_importance = np.mean(all_attributions, axis=0)\n",
    "    \n",
    "    # Feature names (same as in atom_features function)\n",
    "    feature_names = [\n",
    "        'Atomic number', 'Degree', 'Formal charge', 'Radical electrons', 'Is aromatic',\n",
    "        'Explicit valence', 'Implicit valence', 'Total valence', 'Implicit Hs',\n",
    "        'Hybridization', 'Total Hs', 'In ring', 'In ring size 3', 'In ring size 4',\n",
    "        'In ring size 5', 'In ring size 6', 'In ring size 7', 'Chiral tag', 'Mass',\n",
    "        'LogP contribution'\n",
    "    ]\n",
    "    \n",
    "    # Sort by importance\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': feature_importance\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    return importance_df\n",
    "\n",
    "# Create output subfolders\n",
    "viz_dir = os.path.join(output_dir, 'molecule_viz')\n",
    "os.makedirs(viz_dir, exist_ok=True)\n",
    "\n",
    "top_molecules_dir = os.path.join(output_dir, 'top_molecules')\n",
    "os.makedirs(top_molecules_dir, exist_ok=True)\n",
    "\n",
    "# Process all molecules in the dataset\n",
    "molecule_importances = []\n",
    "\n",
    "# Process a subset of molecules (limit to 100 for efficiency)\n",
    "num_molecules = min(100, len(train_data))\n",
    "\n",
    "for i, graph in enumerate(tqdm(train_data[:num_molecules], desc=\"Processing molecules\")):\n",
    "    try:\n",
    "        smiles = graph.smiles\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is None:\n",
    "            continue\n",
    "            \n",
    "        # Get prediction score\n",
    "        graph_batch = graph.clone()\n",
    "        graph_batch.batch = torch.zeros(graph.x.shape[0], dtype=torch.long)\n",
    "        pred_score = wrapper_model(graph.x, graph.edge_index, graph.edge_attr, graph_batch.batch).item()\n",
    "        \n",
    "        # Get node and edge importance\n",
    "        node_importance, edge_importance = interpret_molecule(graph)\n",
    "        \n",
    "        # Calculate importance metrics\n",
    "        avg_importance = np.mean(node_importance)\n",
    "        max_importance = np.max(node_importance)\n",
    "        \n",
    "        # Create a safe filename from the SMILES\n",
    "        safe_filename = create_safe_filename(smiles)\n",
    "        \n",
    "        # Save molecule image with importance visualization\n",
    "        img_data = visualize_molecule_importance(smiles, node_importance, mol)\n",
    "        with open(os.path.join(viz_dir, safe_filename), \"wb\") as f:\n",
    "            f.write(img_data)\n",
    "        \n",
    "        # Add to our results dataframe\n",
    "        molecule_importances.append({\n",
    "            'Index': i+1,\n",
    "            'SMILES': smiles,\n",
    "            'Prediction': pred_score,\n",
    "            'True_Label': graph.y.item(),\n",
    "            'Average_Importance': avg_importance,\n",
    "            'Max_Importance': max_importance,\n",
    "            'Most_Important_Atom_Index': np.argmax(node_importance),\n",
    "            'Filename': safe_filename\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing molecule {i+1}: {e}\")\n",
    "\n",
    "# Create and save dataframe of molecule importances\n",
    "importance_df = pd.DataFrame(molecule_importances)\n",
    "importance_df.to_excel(os.path.join(output_dir, 'molecule_importances.xlsx'), index=False)\n",
    "\n",
    "# Create feature importance analysis\n",
    "feature_imp_df = analyze_feature_importance(train_data[:num_molecules])\n",
    "feature_imp_df.to_excel(os.path.join(output_dir, 'feature_importance.xlsx'), index=False)\n",
    "\n",
    "# Plot and save feature importance graph\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Importance', y='Feature', data=feature_imp_df.head(15))\n",
    "plt.title('Top 15 Important Atom Features (TransformerGCN Analysis)')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, 'feature_importance.png'), dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# Find the top 40 molecules by importance\n",
    "top_molecules_count = min(40, len(importance_df))\n",
    "top_mols = importance_df.sort_values('Average_Importance', ascending=False).head(top_molecules_count)\n",
    "\n",
    "# Create a top SMILES CSV file\n",
    "with open(os.path.join(output_dir, \"top_smiles_cancer.csv\"), \"w\") as f:\n",
    "    f.write(\"Rank,MoleculeIndex,SMILES,Prediction,TrueLabel,AverageImportance,Filename\\n\")\n",
    "    for j, (_, r) in enumerate(top_mols.iterrows()):\n",
    "        f.write(f\"{j+1},{r['Index']},{r['SMILES']},{r['Prediction']:.4f},{r['True_Label']:.0f},{r['Average_Importance']:.4f},{r['Filename']}\\n\")\n",
    "\n",
    "# Create a higher-quality version of the top molecules (with titles)\n",
    "for i, (_, row) in enumerate(tqdm(top_mols.iterrows(), desc=\"Creating high-quality top molecule images\")):\n",
    "    try:\n",
    "        # Get the molecule and its data\n",
    "        idx = row['Index']\n",
    "        smiles = row['SMILES']\n",
    "        pred = row['Prediction']\n",
    "        true_label = row['True_Label']\n",
    "        \n",
    "        # Get the corresponding graph data\n",
    "        graph = train_data[idx-1]  # -1 because indices start at 1 in our data\n",
    "        \n",
    "        # Re-interpret the molecule for visualization\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is None:\n",
    "            continue\n",
    "            \n",
    "        # Get node importance\n",
    "        node_importance, _ = interpret_molecule(graph)\n",
    "        \n",
    "        # Create a title with info\n",
    "        title = f\"#{idx}: Pred={pred:.3f}, True={true_label:.0f}, Importance={row['Average_Importance']:.3f}\"\n",
    "        \n",
    "        # Create a high-quality visualization\n",
    "        img_data = visualize_molecule_importance(smiles, node_importance, mol, title=title, size=(800, 800))\n",
    "        \n",
    "        # Use the same filename as before\n",
    "        safe_filename = row['Filename']\n",
    "        \n",
    "        # Save to the top molecules directory\n",
    "        with open(os.path.join(top_molecules_dir, safe_filename), \"wb\") as f:\n",
    "            f.write(img_data)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating top molecule image for rank {i+1}: {e}\")\n",
    "\n",
    "# Create node importance analysis\n",
    "node_importances = []\n",
    "for i, graph in enumerate(tqdm(train_data[:num_molecules], desc=\"Analyzing node types\")):\n",
    "    try:\n",
    "        smiles = graph.smiles\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is None:\n",
    "            continue\n",
    "            \n",
    "        node_importance, _ = interpret_molecule(graph)\n",
    "        \n",
    "        # Get atom types and their importance\n",
    "        for atom_idx, importance in enumerate(node_importance):\n",
    "            atom_symbol = mol.GetAtomWithIdx(atom_idx).GetSymbol()\n",
    "            node_importances.append({\n",
    "                'Molecule_Index': i+1,\n",
    "                'SMILES': smiles,\n",
    "                'Atom_Index': atom_idx,\n",
    "                'Atom_Symbol': atom_symbol,\n",
    "                'Importance': importance\n",
    "            })\n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing node for molecule {i+1}: {e}\")\n",
    "\n",
    "# Create and save node importance dataframe\n",
    "node_imp_df = pd.DataFrame(node_importances)\n",
    "node_imp_df.to_excel(os.path.join(output_dir, 'node_importances.xlsx'), index=False)\n",
    "\n",
    "# Analyze importance by atom type\n",
    "atom_importance = node_imp_df.groupby('Atom_Symbol')['Importance'].mean().reset_index()\n",
    "atom_importance = atom_importance.sort_values('Importance', ascending=False)\n",
    "atom_importance.to_excel(os.path.join(output_dir, 'atom_type_importance.xlsx'), index=False)\n",
    "\n",
    "# Plot atom type importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Importance', y='Atom_Symbol', data=atom_importance)\n",
    "plt.title('Average Importance by Atom Type (TransformerGCN Analysis)')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, 'atom_type_importance.png'), dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# Create a color-coded Excel file with molecule importances\n",
    "try:\n",
    "    from openpyxl import load_workbook\n",
    "    from openpyxl.styles import PatternFill\n",
    "    from openpyxl.styles.differential import DifferentialStyle\n",
    "    from openpyxl.formatting.rule import ColorScaleRule\n",
    "    \n",
    "    color_excel_path = os.path.join(output_dir, 'molecule_importances_colored.xlsx')\n",
    "    importance_df.to_excel(color_excel_path, index=False)\n",
    "    \n",
    "    # Open the workbook and add color scales\n",
    "    wb = load_workbook(color_excel_path)\n",
    "    ws = wb.active\n",
    "    \n",
    "    # Add color scale to Average_Importance column\n",
    "    col_idx = importance_df.columns.get_loc(\"Average_Importance\") + 1  # +1 because Excel is 1-indexed\n",
    "    col_letter = ws.cell(1, col_idx).column_letter\n",
    "    \n",
    "    ws.conditional_formatting.add(\n",
    "        f\"{col_letter}2:{col_letter}{len(importance_df)+1}\",\n",
    "        ColorScaleRule(\n",
    "            start_type='min', start_color='FFFFFF',\n",
    "            mid_type='percentile', mid_value=50, mid_color='FFFF00',\n",
    "            end_type='max', end_color='FF0000'\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Add color scale to Max_Importance column\n",
    "    col_idx = importance_df.columns.get_loc(\"Max_Importance\") + 1\n",
    "    col_letter = ws.cell(1, col_idx).column_letter\n",
    "    \n",
    "    ws.conditional_formatting.add(\n",
    "        f\"{col_letter}2:{col_letter}{len(importance_df)+1}\",\n",
    "        ColorScaleRule(\n",
    "            start_type='min', start_color='FFFFFF',\n",
    "            mid_type='percentile', mid_value=50, mid_color='FFFF00',\n",
    "            end_type='max', end_color='FF0000'\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Save the workbook\n",
    "    wb.save(color_excel_path)\n",
    "    print(f\"Created color-coded Excel file: {color_excel_path}\")\n",
    "except ImportError:\n",
    "    print(\"openpyxl not available - skipping colored Excel creation\")\n",
    "\n",
    "print(f\"✅ Analysis complete! All results saved to {output_dir}\")\n",
    "print(f\"📊 Key files generated:\")\n",
    "print(f\" - Molecule visualizations with SMILES filenames: {viz_dir}/*.png\")\n",
    "print(f\" - Top 40 molecules: {top_molecules_dir}/*.png\")\n",
    "print(f\" - Top molecules SMILES list: {os.path.join(output_dir, 'top_smiles.csv')}\")\n",
    "print(f\" - Feature importance analysis: {os.path.join(output_dir, 'feature_importance.xlsx')}\")\n",
    "print(f\" - Molecule importance spreadsheet: {os.path.join(output_dir, 'molecule_importances.xlsx')}\")\n",
    "print(f\" - Atom type importance: {os.path.join(output_dir, 'atom_type_importance.png')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5300d6-2936-4f13-b96c-9767e29cb685",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

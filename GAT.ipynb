{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "319dfdb9-260e-4d84-a539-a017b67e9fd5",
   "metadata": {},
   "source": [
    "# Model Development"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3e5cb7-e7d5-47fa-a7b4-35cf30e53fa2",
   "metadata": {},
   "source": [
    "- Train/Test Split > Tuning > Cross Verification > Training > Testing > Model Saved\n",
    "- Traing Loss and Training AUC Plot and Test ROC plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa1dd52-ca88-426e-98b9-30f14f7c569f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined_gat_pipeline_optuna_cv.py\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv, global_mean_pool\n",
    "from torch_geometric.data import DataLoader, Data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import rdMolDescriptors\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, accuracy_score, matthews_corrcoef,\n",
    "    cohen_kappa_score, brier_score_loss, confusion_matrix,\n",
    "    precision_score, f1_score, roc_curve\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "\n",
    "# Ensure output directory\n",
    "output_dir = 'GAT_result'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Expanded atom features\n",
    "def atom_features(atom):\n",
    "    return torch.tensor([\n",
    "        # Basic properties\n",
    "        atom.GetAtomicNum(),                     # Atomic number\n",
    "        atom.GetDegree(),                        # Number of bonded neighbors\n",
    "        atom.GetFormalCharge(),                  # Formal charge\n",
    "        atom.GetNumRadicalElectrons(),           # Number of radical electrons\n",
    "        int(atom.GetIsAromatic()),               # Aromaticity flag\n",
    "        \n",
    "        # Extended properties\n",
    "        atom.GetExplicitValence(),               # Explicit valence\n",
    "        atom.GetImplicitValence(),               # Implicit valence\n",
    "        atom.GetTotalValence(),                  # Total valence\n",
    "        atom.GetNumImplicitHs(),                 # Number of implicit hydrogens\n",
    "        atom.GetHybridization(),                 # Hybridization state\n",
    "        atom.GetTotalNumHs(),                    # Total number of hydrogens\n",
    "        \n",
    "        # Topological properties\n",
    "        int(atom.IsInRing()),                    # Whether the atom is in a ring\n",
    "        int(atom.IsInRingSize(3)),               # Whether in 3-membered ring\n",
    "        int(atom.IsInRingSize(4)),               # Whether in 4-membered ring\n",
    "        int(atom.IsInRingSize(5)),               # Whether in 5-membered ring\n",
    "        int(atom.IsInRingSize(6)),               # Whether in 6-membered ring\n",
    "        int(atom.IsInRingSize(7)),               # Whether in 7-membered ring\n",
    "        \n",
    "        # Electronic properties\n",
    "        atom.GetChiralTag(),                     # Chirality\n",
    "        atom.GetMass(),                          # Atomic mass\n",
    "        Chem.rdMolDescriptors.CalcCrippenDescriptors(\n",
    "            Chem.MolFromSmiles(f\"[{atom.GetSymbol()}]\"))[0]  # LogP contribution\n",
    "    ], dtype=torch.float)\n",
    "\n",
    "# Molecule to PyTorch Geometric graph\n",
    "def mol_to_graph(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None: return None\n",
    "    atoms = [atom_features(atom) for atom in mol.GetAtoms()]\n",
    "    if not atoms: return None\n",
    "    x = torch.stack(atoms, dim=0)\n",
    "    edge_index = []\n",
    "    for bond in mol.GetBonds():\n",
    "        i, j = bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()\n",
    "        edge_index += [[i, j], [j, i]]\n",
    "    edge_index = torch.tensor(edge_index).t().contiguous()\n",
    "    edge_attr = [[bond.GetBondTypeAsDouble(), bond.GetIsConjugated(), bond.IsInRing()] * 2 for bond in mol.GetBonds()]\n",
    "    edge_attr = torch.tensor(edge_attr, dtype=torch.float).reshape(-1, 3)\n",
    "    return Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "\n",
    "# Load dataset\n",
    "def load_dataset(path, smiles_col='Smiles', target_col='Target'):\n",
    "    df = pd.read_excel(path)\n",
    "    graphs = []\n",
    "    for _, row in df.iterrows():\n",
    "        if pd.isna(row[smiles_col]) or pd.isna(row[target_col]):\n",
    "            continue\n",
    "        g = mol_to_graph(str(row[smiles_col]))\n",
    "        if g is not None:\n",
    "            g.y = torch.tensor([row[target_col]], dtype=torch.float)\n",
    "            g.smiles = row[smiles_col]\n",
    "            graphs.append(g)\n",
    "    return graphs\n",
    "\n",
    "# GAT model - Replacing GCN with GAT\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers=3, heads=4, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        \n",
    "        # First layer has in_channels -> hidden_channels*heads\n",
    "        self.convs.append(GATConv(in_channels, hidden_channels, heads=heads, dropout=dropout))\n",
    "        \n",
    "        # Middle layers (hidden -> hidden)\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(GATConv(hidden_channels * heads, hidden_channels, heads=heads, dropout=dropout))\n",
    "        \n",
    "        # Last layer (for prediction)\n",
    "        self.convs.append(GATConv(hidden_channels * heads, out_channels, heads=1, concat=False, dropout=dropout))\n",
    "        \n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, batch):\n",
    "        for conv in self.convs[:-1]:\n",
    "            x = conv(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.convs[-1](x, edge_index)\n",
    "        return global_mean_pool(x, batch)\n",
    "\n",
    "# Training and AUC\n",
    "def train(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in loader:\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index, data.edge_attr, data.batch).squeeze()\n",
    "        loss = criterion(torch.sigmoid(out), data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def compute_auc(model, loader):\n",
    "    model.eval()\n",
    "    y_true, y_probs = [], []\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            out = model(data.x, data.edge_index, data.edge_attr, data.batch).squeeze()\n",
    "            y_probs.extend(torch.sigmoid(out).cpu().numpy())\n",
    "            y_true.extend(data.y.cpu().numpy())\n",
    "    return roc_auc_score(y_true, y_probs)\n",
    "\n",
    "# Model evaluation\n",
    "def get_metrics(model, loader):\n",
    "    model.eval()\n",
    "    y_true, y_probs = [], []\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            out = model(data.x, data.edge_index, data.edge_attr, data.batch).squeeze()\n",
    "            y_probs.extend(torch.sigmoid(out).cpu().numpy())\n",
    "            y_true.extend(data.y.cpu().numpy())\n",
    "    y_preds = (np.array(y_probs) > 0.5).astype(int)\n",
    "    return {\n",
    "        'AUC': roc_auc_score(y_true, y_probs),\n",
    "        'Accuracy': accuracy_score(y_true, y_preds),\n",
    "        'MCC': matthews_corrcoef(y_true, y_preds),\n",
    "        'Precision': precision_score(y_true, y_preds),\n",
    "        'F1': f1_score(y_true, y_preds),\n",
    "        'Kappa': cohen_kappa_score(y_true, y_preds),\n",
    "        'Brier': brier_score_loss(y_true, y_probs),\n",
    "    }\n",
    "\n",
    "# Globals for Optuna\n",
    "trial_train_loader = None\n",
    "trial_val_loader = None\n",
    "\n",
    "# Optuna objective\n",
    "def objective(trial):\n",
    "    hidden_channels = trial.suggest_int(\"hidden_channels\", 32, 128)\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 2, 4)\n",
    "    heads = trial.suggest_int(\"heads\", 2, 8)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)\n",
    "\n",
    "    model = GAT(20, hidden_channels, 1, num_layers, heads, dropout)  # Updated model to GAT\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = torch.nn.BCELoss()\n",
    "\n",
    "    for epoch in range(10):\n",
    "        train(model, trial_train_loader, optimizer, criterion)\n",
    "\n",
    "    return compute_auc(model, trial_val_loader)\n",
    "\n",
    "def main():\n",
    "    global trial_train_loader, trial_val_loader\n",
    "\n",
    "    graphs = load_dataset(\"dataset_main.xlsx\")\n",
    "    train_data, test_data = train_test_split(graphs, test_size=0.2, random_state=42)\n",
    "    trial_train, trial_val = train_test_split(train_data, test_size=0.2, random_state=42)\n",
    "    trial_train_loader = DataLoader(trial_train, batch_size=32, shuffle=True)\n",
    "    trial_val_loader = DataLoader(trial_val, batch_size=32)\n",
    "\n",
    "    # Optuna tuning\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=30)\n",
    "    best_params = study.best_trial.params\n",
    "    json.dump(best_params, open(os.path.join(output_dir, \"study_best_params_gat.json\"), \"w\"), indent=4)\n",
    "\n",
    "    # 10-fold cross-validation (mean metrics only)\n",
    "    kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "    metrics_list = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(train_data), start=1):\n",
    "        model = GAT(20, best_params[\"hidden_channels\"], 1, \n",
    "                    best_params[\"num_layers\"], best_params[\"heads\"], best_params[\"dropout\"])\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=best_params[\"lr\"])\n",
    "        criterion = torch.nn.BCELoss()\n",
    "\n",
    "        train_fold = [train_data[i] for i in train_idx]\n",
    "        val_fold = [train_data[i] for i in val_idx]\n",
    "        loader_train = DataLoader(train_fold, batch_size=32, shuffle=True)\n",
    "        loader_val = DataLoader(val_fold, batch_size=32)\n",
    "\n",
    "        for epoch in range(30):\n",
    "            train(model, loader_train, optimizer, criterion)\n",
    "\n",
    "        metrics = get_metrics(model, loader_val)\n",
    "        metrics_list.append(metrics)\n",
    "\n",
    "    # Save and print mean of cross-validation metrics\n",
    "    df_cv = pd.DataFrame(metrics_list)\n",
    "    mean_metrics = df_cv.mean()\n",
    "    df_cv.loc['mean'] = mean_metrics\n",
    "    df_cv.tail(1).to_excel(os.path.join(output_dir, \"cv_metrics_mean.xlsx\"), index=False)\n",
    "\n",
    "    print(\"\\n📊 10-Fold Cross-Validation Mean Metrics:\")\n",
    "    for key, value in mean_metrics.items():\n",
    "        print(f\"{key}: {value:.4f}\")\n",
    "\n",
    "    # Final training on full training set\n",
    "    full_train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(test_data, batch_size=32)\n",
    "\n",
    "    final_model = GAT(20, best_params[\"hidden_channels\"], 1, \n",
    "                     best_params[\"num_layers\"], best_params[\"heads\"], best_params[\"dropout\"])\n",
    "    optimizer = torch.optim.Adam(final_model.parameters(), lr=best_params[\"lr\"])\n",
    "    criterion = torch.nn.BCELoss()\n",
    "\n",
    "    losses, aucs = [], []\n",
    "    for epoch in range(1, 101):\n",
    "        loss = train(final_model, full_train_loader, optimizer, criterion)\n",
    "        auc = compute_auc(final_model, full_train_loader)\n",
    "        losses.append(loss)\n",
    "        aucs.append(auc)\n",
    "        print(f\"[Final Train] Epoch {epoch} | Loss: {loss:.4f} | AUC: {auc:.4f}\")\n",
    "\n",
    "    torch.save(final_model.state_dict(), os.path.join(output_dir, \"gat_best_model.pth\"))\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(losses, label=\"Loss\")\n",
    "    plt.title(\"Final Training Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.savefig(os.path.join(output_dir, \"final_training_loss.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(aucs, label=\"AUC\", color='red')\n",
    "    plt.title(\"Final Training AUC\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"AUC\")\n",
    "    plt.savefig(os.path.join(output_dir, \"final_training_auc.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # Final test set evaluation\n",
    "    test_metrics = get_metrics(final_model, test_loader)\n",
    "    pd.DataFrame([test_metrics]).to_excel(os.path.join(output_dir, \"test_metrics.xlsx\"), index=False)\n",
    "\n",
    "    print(\"\\n🧪 Final Test Set Metrics:\")\n",
    "    for key, value in test_metrics.items():\n",
    "        print(f\"{key}: {value:.4f}\")\n",
    "\n",
    "    # ROC Curve\n",
    "    model = final_model\n",
    "    model.eval()\n",
    "    y_true, y_probs = [], []\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            out = model(data.x, data.edge_index, data.edge_attr, data.batch).squeeze()\n",
    "            y_probs.extend(torch.sigmoid(out).cpu().numpy())\n",
    "            y_true.extend(data.y.cpu().numpy())\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_probs)\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, label=f\"AUC = {test_metrics['AUC']:.2f}\")\n",
    "    plt.plot([0, 1], [0, 1], '--', color='gray')\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"Test ROC Curve\")\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(output_dir, \"test_roc.png\"))\n",
    "    plt.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942b5512-774c-477d-adde-e9e0a2946aaf",
   "metadata": {},
   "source": [
    "## Confusion matrix creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e06574-c6e9-4aec-b8fe-e2c68ce8bebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from torch_geometric.data import DataLoader\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import rdMolDescriptors\n",
    "\n",
    "# Reuse the necessary functions from cell1 without running the training\n",
    "def atom_features(atom):\n",
    "    return torch.tensor([\n",
    "        # Basic properties\n",
    "        atom.GetAtomicNum(),\n",
    "        atom.GetDegree(),\n",
    "        atom.GetFormalCharge(),\n",
    "        atom.GetNumRadicalElectrons(),\n",
    "        int(atom.GetIsAromatic()),\n",
    "        \n",
    "        # Extended properties\n",
    "        atom.GetExplicitValence(),\n",
    "        atom.GetImplicitValence(),\n",
    "        atom.GetTotalValence(),\n",
    "        atom.GetNumImplicitHs(),\n",
    "        atom.GetHybridization(),\n",
    "        atom.GetTotalNumHs(),\n",
    "        \n",
    "        # Topological properties\n",
    "        int(atom.IsInRing()),\n",
    "        int(atom.IsInRingSize(3)),\n",
    "        int(atom.IsInRingSize(4)),\n",
    "        int(atom.IsInRingSize(5)),\n",
    "        int(atom.IsInRingSize(6)),\n",
    "        int(atom.IsInRingSize(7)),\n",
    "        \n",
    "        # Electronic properties\n",
    "        atom.GetChiralTag(),\n",
    "        atom.GetMass(),\n",
    "        Chem.rdMolDescriptors.CalcCrippenDescriptors(\n",
    "            Chem.MolFromSmiles(f\"[{atom.GetSymbol()}]\"))[0]\n",
    "    ], dtype=torch.float)\n",
    "\n",
    "def mol_to_graph(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None: return None\n",
    "    atoms = [atom_features(atom) for atom in mol.GetAtoms()]\n",
    "    if not atoms: return None\n",
    "    x = torch.stack(atoms, dim=0)\n",
    "    edge_index = []\n",
    "    for bond in mol.GetBonds():\n",
    "        i, j = bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()\n",
    "        edge_index += [[i, j], [j, i]]\n",
    "    edge_index = torch.tensor(edge_index).t().contiguous()\n",
    "    edge_attr = [[bond.GetBondTypeAsDouble(), bond.GetIsConjugated(), bond.IsInRing()] * 2 for bond in mol.GetBonds()]\n",
    "    edge_attr = torch.tensor(edge_attr, dtype=torch.float).reshape(-1, 3)\n",
    "    return Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "\n",
    "def load_dataset(path, smiles_col='Smiles', target_col='Target'):\n",
    "    df = pd.read_excel(path)\n",
    "    graphs = []\n",
    "    for _, row in df.iterrows():\n",
    "        if pd.isna(row[smiles_col]) or pd.isna(row[target_col]):\n",
    "            continue\n",
    "        g = mol_to_graph(str(row[smiles_col]))\n",
    "        if g is not None:\n",
    "            g.y = torch.tensor([row[target_col]], dtype=torch.float)\n",
    "            g.smiles = row[smiles_col]\n",
    "            graphs.append(g)\n",
    "    return graphs\n",
    "\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers=3, heads=4, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        \n",
    "        # First layer has in_channels -> hidden_channels*heads\n",
    "        self.convs.append(GATConv(in_channels, hidden_channels, heads=heads, dropout=dropout))\n",
    "        \n",
    "        # Middle layers (hidden -> hidden)\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(GATConv(hidden_channels * heads, hidden_channels, heads=heads, dropout=dropout))\n",
    "        \n",
    "        # Last layer (for prediction)\n",
    "        self.convs.append(GATConv(hidden_channels * heads, out_channels, heads=1, concat=False, dropout=dropout))\n",
    "        \n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, batch):\n",
    "        for conv in self.convs[:-1]:\n",
    "            x = conv(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.convs[-1](x, edge_index)\n",
    "        return global_mean_pool(x, batch)\n",
    "\n",
    "# Main execution for generating confusion matrix\n",
    "def generate_confusion_matrix():\n",
    "    output_dir = 'GAT_result'\n",
    "    \n",
    "    # Load the best parameters\n",
    "    try:\n",
    "        with open(os.path.join(output_dir, \"study_best_params_gat.json\"), \"r\") as f:\n",
    "            best_params = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(\"Best parameters file not found. Using default parameters.\")\n",
    "        best_params = {\n",
    "            \"hidden_channels\": 64,\n",
    "            \"num_layers\": 3,\n",
    "            \"heads\": 4,\n",
    "            \"dropout\": 0.3,\n",
    "            \"lr\": 0.001\n",
    "        }\n",
    "    \n",
    "    # Load dataset and create test loader\n",
    "    graphs = load_dataset(\"dataset_main.xlsx\")\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    _, test_data = train_test_split(graphs, test_size=0.2, random_state=42)\n",
    "    test_loader = DataLoader(test_data, batch_size=32)\n",
    "    \n",
    "    # Initialize model with best parameters\n",
    "    model = GAT(20, best_params[\"hidden_channels\"], 1, \n",
    "               best_params[\"num_layers\"], best_params[\"heads\"], best_params[\"dropout\"])\n",
    "    \n",
    "    # Load trained model weights\n",
    "    try:\n",
    "        model.load_state_dict(torch.load(os.path.join(output_dir, \"gat_best_model.pth\")))\n",
    "        print(\"Model loaded successfully!\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Model file not found. Cannot generate confusion matrix without trained model.\")\n",
    "        return\n",
    "    \n",
    "    # Generate predictions\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    y_probs = []\n",
    "    molecule_smiles = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            out = model(data.x, data.edge_index, data.edge_attr, data.batch).squeeze()\n",
    "            probs = torch.sigmoid(out).cpu().numpy()\n",
    "            preds = (probs > 0.5).astype(int)\n",
    "            \n",
    "            y_probs.extend(probs)\n",
    "            y_pred.extend(preds)\n",
    "            y_true.extend(data.y.cpu().numpy())\n",
    "            molecule_smiles.extend([g.smiles for g in data.to_data_list()])\n",
    "    \n",
    "    # Create confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    class_names = ['Negative', 'Positive']\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
    "    disp.plot(cmap=plt.cm.Blues, values_format='d')\n",
    "    plt.title('Confusion Matrix for GAT Model')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, \"confusion_matrix.png\"))\n",
    "    \n",
    "    # Create a prettier confusion matrix with seaborn\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.title('Confusion Matrix for GAT Model')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, \"confusion_matrix_seaborn.png\"))\n",
    "    \n",
    "    # Create a dataframe with predictions for analysis\n",
    "    results_df = pd.DataFrame({\n",
    "        'SMILES': molecule_smiles,\n",
    "        'True_Label': y_true,\n",
    "        'Predicted_Label': y_pred,\n",
    "        'Prediction_Probability': y_probs\n",
    "    })\n",
    "    \n",
    "    # Save predictions to Excel\n",
    "    results_df.to_excel(os.path.join(output_dir, \"prediction_results.xlsx\"), index=False)\n",
    "    \n",
    "    # Calculate metrics for each class\n",
    "    TN, FP, FN, TP = cm.ravel()\n",
    "    \n",
    "    # True Positive Rate (Sensitivity or Recall)\n",
    "    TPR = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "    \n",
    "    # True Negative Rate (Specificity)\n",
    "    TNR = TN / (TN + FP) if (TN + FP) > 0 else 0\n",
    "    \n",
    "    # False Positive Rate\n",
    "    FPR = FP / (FP + TN) if (FP + TN) > 0 else 0\n",
    "    \n",
    "    # False Negative Rate\n",
    "    FNR = FN / (TP + FN) if (TP + FN) > 0 else 0\n",
    "    \n",
    "    # Precision\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "    \n",
    "    # F1 Score\n",
    "    f1 = 2 * TP / (2 * TP + FP + FN) if (2 * TP + FP + FN) > 0 else 0\n",
    "    \n",
    "    # Print detailed metrics\n",
    "    print(\"\\n===== Confusion Matrix =====\")\n",
    "    print(f\"True Negative (TN): {TN}\")\n",
    "    print(f\"False Positive (FP): {FP}\")\n",
    "    print(f\"False Negative (FN): {FN}\")\n",
    "    print(f\"True Positive (TP): {TP}\")\n",
    "    print(\"\\n===== Detailed Metrics =====\")\n",
    "    print(f\"True Positive Rate (Sensitivity/Recall): {TPR:.4f}\")\n",
    "    print(f\"True Negative Rate (Specificity): {TNR:.4f}\")\n",
    "    print(f\"False Positive Rate: {FPR:.4f}\")\n",
    "    print(f\"False Negative Rate: {FNR:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    \n",
    "    # Save metrics to file\n",
    "    metrics_df = pd.DataFrame({\n",
    "        'Metric': ['TN', 'FP', 'FN', 'TP', 'Sensitivity/Recall', 'Specificity', \n",
    "                  'False Positive Rate', 'False Negative Rate', 'Precision', 'F1 Score'],\n",
    "        'Value': [TN, FP, FN, TP, TPR, TNR, FPR, FNR, precision, f1]\n",
    "    })\n",
    "    metrics_df.to_excel(os.path.join(output_dir, \"detailed_metrics.xlsx\"), index=False)\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# Run the function\n",
    "if __name__ == \"__main__\":\n",
    "    generate_confusion_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cb6621-4d4f-4948-8d5a-c6b2197799f4",
   "metadata": {},
   "source": [
    "# External Dataset Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f646e5-d239-4661-b744-ff6f61f68417",
   "metadata": {},
   "source": [
    "## Phytos_Results.xlsx\n",
    "- Predicted Results saved to excel\n",
    "- ROC plot and Confusion Matrix plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d1d95f-9a96-4b80-bb87-808018714012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: External Dataset Evaluation with Fixed GAT Model Loading\n",
    "\n",
    "# Import additional required libraries\n",
    "import seaborn as sns\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "import warnings\n",
    "\n",
    "# Ensure output directory exists\n",
    "output_dir = 'GAT_result'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# First, examine the saved model to understand its architecture\n",
    "saved_state_dict = torch.load(os.path.join(output_dir, \"gat_best_model.pth\"))\n",
    "\n",
    "# Print the keys to see the model structure\n",
    "print(\"Saved model layers:\")\n",
    "for key in saved_state_dict.keys():\n",
    "    if 'weight' in key:\n",
    "        print(f\"{key}: {saved_state_dict[key].shape}\")\n",
    "\n",
    "# Function to load external dataset\n",
    "def load_external_dataset(path, smiles_col='Smiles', target_col='Target'):\n",
    "    df = pd.read_excel(path)\n",
    "    graphs = []\n",
    "    molecules = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        if pd.isna(row[smiles_col]):\n",
    "            continue\n",
    "            \n",
    "        g = mol_to_graph(str(row[smiles_col]))\n",
    "        if g is not None:\n",
    "            # If target is available, use it, otherwise set to None\n",
    "            if target_col in df.columns and not pd.isna(row[target_col]):\n",
    "                g.y = torch.tensor([row[target_col]], dtype=torch.float)\n",
    "            else:\n",
    "                g.y = None\n",
    "                \n",
    "            g.smiles = row[smiles_col]\n",
    "            graphs.append(g)\n",
    "            molecules.append(row.to_dict())\n",
    "    \n",
    "    return graphs, pd.DataFrame(molecules)\n",
    "\n",
    "# Custom model class that matches the saved GAT architecture\n",
    "class GATForLoading(torch.nn.Module):\n",
    "    def __init__(self, in_features, hidden_size, heads):\n",
    "        super().__init__()\n",
    "        # Initialize with the architecture from the saved model\n",
    "        # Using parameters based on the saved state shapes\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.convs.append(GATConv(in_features, hidden_size, heads=heads))  # Input features to hidden\n",
    "        self.convs.append(GATConv(hidden_size * heads, hidden_size, heads=heads))  # Hidden to hidden\n",
    "        self.convs.append(GATConv(hidden_size * heads, hidden_size, heads=heads))  # Hidden to hidden  \n",
    "        self.convs.append(GATConv(hidden_size * heads, 1, heads=1, concat=False))  # Hidden to output\n",
    "        self.dropout = 0.3  # Default value, will be overridden by loaded state\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, batch):\n",
    "        for conv in self.convs[:-1]:\n",
    "            x = conv(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.convs[-1](x, edge_index)\n",
    "        return global_mean_pool(x, batch)\n",
    "\n",
    "# Get hidden size and heads from saved model\n",
    "# Analyze the saved weights to infer parameters\n",
    "hidden_size = None\n",
    "heads = None\n",
    "in_features = 20  # Our updated model has 20 input features\n",
    "\n",
    "# Try to determine parameters from state dict shape\n",
    "for key in saved_state_dict.keys():\n",
    "    if 'weight' in key and 'convs.0' in key:\n",
    "        # For GAT, weight shape would be different due to attention mechanism\n",
    "        # Need to infer both hidden_size and heads\n",
    "        shape = saved_state_dict[key].shape\n",
    "        if len(shape) == 3:  # GAT weight shape might be [heads, out_channels, in_channels]\n",
    "            heads = shape[0]\n",
    "            hidden_size = shape[1]\n",
    "            break\n",
    "\n",
    "if hidden_size is None or heads is None:\n",
    "    # Default if we couldn't determine from saved state\n",
    "    hidden_size = 64\n",
    "    heads = 4\n",
    "    print(f\"Warning: Could not determine architecture from saved model. Using defaults: hidden_size={hidden_size}, heads={heads}\")\n",
    "else:\n",
    "    print(f\"Detected hidden size: {hidden_size}, heads: {heads}\")\n",
    "\n",
    "# Create the model with the correct architecture\n",
    "model = GATForLoading(in_features=in_features, hidden_size=hidden_size, heads=heads)\n",
    "\n",
    "# Try to load the saved state\n",
    "try:\n",
    "    model.load_state_dict(torch.load(os.path.join(output_dir, \"gat_best_model.pth\")))\n",
    "    print(\"Successfully loaded model weights\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model weights: {str(e)}\")\n",
    "    print(\"This may happen if the model architecture doesn't match the saved weights.\")\n",
    "    print(\"Make sure you've trained the model with the updated atom features first.\")\n",
    "    # Continue anyway for demonstration purposes\n",
    "    print(\"Continuing with uninitialized model for demonstration...\")\n",
    "\n",
    "# Set to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Load external dataset\n",
    "external_data, external_df = load_external_dataset(\"Phytos_results.xlsx\")\n",
    "# Suppress deprecation warning for DataLoader\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "    external_loader = DataLoader(external_data, batch_size=32)\n",
    "\n",
    "# Get predictions\n",
    "y_true = []\n",
    "y_probs = []\n",
    "smiles_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in external_loader:\n",
    "        try:\n",
    "            out = model(batch.x, batch.edge_index, batch.edge_attr, batch.batch)\n",
    "            # Don't squeeze - handle the dimensionality properly\n",
    "            probs = torch.sigmoid(out).cpu().numpy()\n",
    "            \n",
    "            # Handle both single-item and multi-item batches\n",
    "            if len(probs.shape) == 0:  # Single item case (0-d array)\n",
    "                probs = np.array([probs.item()])\n",
    "            \n",
    "            for i in range(len(probs)):\n",
    "                y_probs.append(probs[i])\n",
    "                \n",
    "                # Get the SMILES safely\n",
    "                if hasattr(batch, 'smiles'):\n",
    "                    if isinstance(batch.smiles, list):\n",
    "                        smiles_list.append(batch.smiles[i])\n",
    "                    else:\n",
    "                        # Handle the case where smiles might be a single string\n",
    "                        smiles_list.append(batch.smiles)\n",
    "                \n",
    "                # Get the true value if it exists\n",
    "                if hasattr(batch, 'y') and batch.y is not None:\n",
    "                    if batch.y.dim() > 0 and i < batch.y.size(0):\n",
    "                        y_true.append(batch.y[i].item())\n",
    "                    elif batch.y.dim() == 0:  # Single item\n",
    "                        y_true.append(batch.y.item())\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing batch: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "# Save predictions to Excel\n",
    "results_df = pd.DataFrame({\n",
    "    'Smiles': smiles_list,\n",
    "    'Predicted_Probability': y_probs,\n",
    "    'Predicted_Class': [1 if p > 0.5 else 0 for p in y_probs]\n",
    "})\n",
    "\n",
    "# If we have true values, add them and calculate metrics\n",
    "if len(y_true) > 0 and len(y_true) == len(y_probs):\n",
    "    results_df['Actual'] = y_true\n",
    "    \n",
    "    # Calculate metrics\n",
    "    y_true_np = np.array(y_true)  # Convert list to numpy array\n",
    "    y_probs_np = np.array(y_probs)\n",
    "    y_preds = (y_probs_np > 0.5).astype(int)\n",
    "    \n",
    "    # Check if we have multiple classes for metrics that require it\n",
    "    unique_classes = np.unique(y_true_np)\n",
    "    has_multiple_classes = len(unique_classes) > 1\n",
    "    \n",
    "    metrics = {\n",
    "        'Accuracy': accuracy_score(y_true_np, y_preds),\n",
    "        'Precision': precision_score(y_true_np, y_preds, zero_division=0),\n",
    "        'F1': f1_score(y_true_np, y_preds, zero_division=0),\n",
    "        'Kappa': cohen_kappa_score(y_true_np, y_preds),\n",
    "        'Brier': brier_score_loss(y_true_np, y_probs_np),\n",
    "    }\n",
    "    \n",
    "    # Only calculate AUC and MCC if we have multiple classes\n",
    "    if has_multiple_classes:\n",
    "        # Suppress the UndefinedMetricWarning\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "            metrics['AUC'] = roc_auc_score(y_true_np, y_probs_np)\n",
    "            metrics['MCC'] = matthews_corrcoef(y_true_np, y_preds)\n",
    "    else:\n",
    "        print(\"\\n⚠️ Warning: Only one class is present in the dataset.\")\n",
    "        print(\"AUC and MCC are not defined in this case and will be reported as N/A.\")\n",
    "        metrics['AUC'] = \"N/A\"\n",
    "        metrics['MCC'] = \"N/A\"\n",
    "    \n",
    "    # Convert N/A values to NaN for Excel export\n",
    "    metrics_for_excel = {k: np.nan if v == \"N/A\" else v for k, v in metrics.items()}\n",
    "    \n",
    "    # Save metrics\n",
    "    pd.DataFrame([metrics_for_excel]).to_excel(os.path.join(output_dir, \"external_metrics.xlsx\"), index=False)\n",
    "    \n",
    "    print(\"\\n🧪 External Dataset Metrics:\")\n",
    "    for key, value in metrics.items():\n",
    "        if isinstance(value, str):\n",
    "            print(f\"{key}: {value}\")\n",
    "        else:\n",
    "            print(f\"{key}: {value:.4f}\")\n",
    "    \n",
    "    # Only generate ROC curve if we have multiple classes\n",
    "    if has_multiple_classes:\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "            fpr, tpr, _ = roc_curve(y_true_np, y_probs_np)\n",
    "            \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.plot(fpr, tpr, label=f\"AUC = {metrics['AUC']:.2f}\" if isinstance(metrics['AUC'], float) else \"AUC = N/A\")\n",
    "        plt.plot([0, 1], [0, 1], '--', color='gray')\n",
    "        plt.xlabel(\"False Positive Rate\")\n",
    "        plt.ylabel(\"True Positive Rate\")\n",
    "        plt.title(\"External Dataset ROC Curve\")\n",
    "        plt.legend()\n",
    "        plt.savefig(os.path.join(output_dir, \"external_roc.png\"))\n",
    "        plt.close()\n",
    "    \n",
    "    # Create confusion matrix\n",
    "    cm = confusion_matrix(y_true_np, y_preds)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.title('Confusion Matrix - External Dataset')\n",
    "    plt.savefig(os.path.join(output_dir, \"external_confusion_matrix.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # Class distribution information\n",
    "    class_counts = np.bincount(y_true_np.astype(int))\n",
    "    print(\"\\n📊 Class Distribution in External Dataset:\")\n",
    "    for i, count in enumerate(class_counts):\n",
    "        print(f\"Class {i}: {count} samples ({count/len(y_true_np)*100:.1f}%)\")\n",
    "\n",
    "# Save all predictions\n",
    "results_df.to_excel(os.path.join(output_dir, \"external_predictions.xlsx\"), index=False)\n",
    "print(f\"\\n💾 Predictions for {len(results_df)} molecules saved to {os.path.join(output_dir, 'external_predictions.xlsx')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c251edbd-3726-4df0-9386-395f6ff0a0b5",
   "metadata": {},
   "source": [
    "## cancer_set.xlsx\n",
    "- Predicted Results saved to excel\n",
    "- ROC plot and Confusion Matrix plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8c9ffa-29b9-4113-9ca5-1b735f0dedf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: External Dataset Evaluation with Fixed GAT Model Loading\n",
    "\n",
    "# Import additional required libraries\n",
    "import seaborn as sns\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "import warnings\n",
    "\n",
    "# Ensure output directory exists\n",
    "output_dir = 'GAT_result'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# First, examine the saved model to understand its architecture\n",
    "saved_state_dict = torch.load(os.path.join(output_dir, \"gat_best_model.pth\"))\n",
    "\n",
    "# Print the keys to see the model structure\n",
    "print(\"Saved model layers:\")\n",
    "for key in saved_state_dict.keys():\n",
    "    if 'weight' in key:\n",
    "        print(f\"{key}: {saved_state_dict[key].shape}\")\n",
    "\n",
    "# Function to load external dataset\n",
    "def load_external_dataset(path, smiles_col='Smiles', target_col='Target'):\n",
    "    df = pd.read_excel(path)\n",
    "    graphs = []\n",
    "    molecules = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        if pd.isna(row[smiles_col]):\n",
    "            continue\n",
    "            \n",
    "        g = mol_to_graph(str(row[smiles_col]))\n",
    "        if g is not None:\n",
    "            # If target is available, use it, otherwise set to None\n",
    "            if target_col in df.columns and not pd.isna(row[target_col]):\n",
    "                g.y = torch.tensor([row[target_col]], dtype=torch.float)\n",
    "            else:\n",
    "                g.y = None\n",
    "                \n",
    "            g.smiles = row[smiles_col]\n",
    "            graphs.append(g)\n",
    "            molecules.append(row.to_dict())\n",
    "    \n",
    "    return graphs, pd.DataFrame(molecules)\n",
    "\n",
    "# Custom model class that matches the saved GAT architecture\n",
    "class GATForLoading(torch.nn.Module):\n",
    "    def __init__(self, in_features, hidden_size, heads):\n",
    "        super().__init__()\n",
    "        # Initialize with the architecture from the saved model\n",
    "        # Using parameters based on the saved state shapes\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.convs.append(GATConv(in_features, hidden_size, heads=heads))  # Input features to hidden\n",
    "        self.convs.append(GATConv(hidden_size * heads, hidden_size, heads=heads))  # Hidden to hidden\n",
    "        self.convs.append(GATConv(hidden_size * heads, hidden_size, heads=heads))  # Hidden to hidden  \n",
    "        self.convs.append(GATConv(hidden_size * heads, 1, heads=1, concat=False))  # Hidden to output\n",
    "        self.dropout = 0.3  # Default value, will be overridden by loaded state\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, batch):\n",
    "        for conv in self.convs[:-1]:\n",
    "            x = conv(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.convs[-1](x, edge_index)\n",
    "        return global_mean_pool(x, batch)\n",
    "\n",
    "# Get hidden size and heads from saved model\n",
    "# Analyze the saved weights to infer parameters\n",
    "hidden_size = None\n",
    "heads = None\n",
    "in_features = 20  # Our updated model has 20 input features\n",
    "\n",
    "# Try to determine parameters from state dict shape\n",
    "for key in saved_state_dict.keys():\n",
    "    if 'weight' in key and 'convs.0' in key:\n",
    "        # For GAT, weight shape would be different due to attention mechanism\n",
    "        # Need to infer both hidden_size and heads\n",
    "        shape = saved_state_dict[key].shape\n",
    "        if len(shape) == 3:  # GAT weight shape might be [heads, out_channels, in_channels]\n",
    "            heads = shape[0]\n",
    "            hidden_size = shape[1]\n",
    "            break\n",
    "\n",
    "if hidden_size is None or heads is None:\n",
    "    # Default if we couldn't determine from saved state\n",
    "    hidden_size = 64\n",
    "    heads = 4\n",
    "    print(f\"Warning: Could not determine architecture from saved model. Using defaults: hidden_size={hidden_size}, heads={heads}\")\n",
    "else:\n",
    "    print(f\"Detected hidden size: {hidden_size}, heads: {heads}\")\n",
    "\n",
    "# Create the model with the correct architecture\n",
    "model = GATForLoading(in_features=in_features, hidden_size=hidden_size, heads=heads)\n",
    "\n",
    "# Try to load the saved state\n",
    "try:\n",
    "    model.load_state_dict(torch.load(os.path.join(output_dir, \"gat_best_model.pth\")))\n",
    "    print(\"Successfully loaded model weights\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model weights: {str(e)}\")\n",
    "    print(\"This may happen if the model architecture doesn't match the saved weights.\")\n",
    "    print(\"Make sure you've trained the model with the updated atom features first.\")\n",
    "    # Continue anyway for demonstration purposes\n",
    "    print(\"Continuing with uninitialized model for demonstration...\")\n",
    "\n",
    "# Set to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Load external dataset\n",
    "external_data, external_df = load_external_dataset(\"cancer_set.xlsx\")\n",
    "# Suppress deprecation warning for DataLoader\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "    external_loader = DataLoader(external_data, batch_size=32)\n",
    "\n",
    "# Get predictions\n",
    "y_true = []\n",
    "y_probs = []\n",
    "smiles_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in external_loader:\n",
    "        try:\n",
    "            out = model(batch.x, batch.edge_index, batch.edge_attr, batch.batch)\n",
    "            # Don't squeeze - handle the dimensionality properly\n",
    "            probs = torch.sigmoid(out).cpu().numpy()\n",
    "            \n",
    "            # Handle both single-item and multi-item batches\n",
    "            if len(probs.shape) == 0:  # Single item case (0-d array)\n",
    "                probs = np.array([probs.item()])\n",
    "            \n",
    "            for i in range(len(probs)):\n",
    "                y_probs.append(probs[i])\n",
    "                \n",
    "                # Get the SMILES safely\n",
    "                if hasattr(batch, 'smiles'):\n",
    "                    if isinstance(batch.smiles, list):\n",
    "                        smiles_list.append(batch.smiles[i])\n",
    "                    else:\n",
    "                        # Handle the case where smiles might be a single string\n",
    "                        smiles_list.append(batch.smiles)\n",
    "                \n",
    "                # Get the true value if it exists\n",
    "                if hasattr(batch, 'y') and batch.y is not None:\n",
    "                    if batch.y.dim() > 0 and i < batch.y.size(0):\n",
    "                        y_true.append(batch.y[i].item())\n",
    "                    elif batch.y.dim() == 0:  # Single item\n",
    "                        y_true.append(batch.y.item())\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing batch: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "# Save predictions to Excel\n",
    "results_df = pd.DataFrame({\n",
    "    'Smiles': smiles_list,\n",
    "    'Predicted_Probability': y_probs,\n",
    "    'Predicted_Class': [1 if p > 0.5 else 0 for p in y_probs]\n",
    "})\n",
    "\n",
    "# If we have true values, add them and calculate metrics\n",
    "if len(y_true) > 0 and len(y_true) == len(y_probs):\n",
    "    results_df['Actual'] = y_true\n",
    "    \n",
    "    # Calculate metrics\n",
    "    y_true_np = np.array(y_true)  # Convert list to numpy array\n",
    "    y_probs_np = np.array(y_probs)\n",
    "    y_preds = (y_probs_np > 0.5).astype(int)\n",
    "    \n",
    "    # Check if we have multiple classes for metrics that require it\n",
    "    unique_classes = np.unique(y_true_np)\n",
    "    has_multiple_classes = len(unique_classes) > 1\n",
    "    \n",
    "    metrics = {\n",
    "        'Accuracy': accuracy_score(y_true_np, y_preds),\n",
    "        'Precision': precision_score(y_true_np, y_preds, zero_division=0),\n",
    "        'F1': f1_score(y_true_np, y_preds, zero_division=0),\n",
    "        'Kappa': cohen_kappa_score(y_true_np, y_preds),\n",
    "        'Brier': brier_score_loss(y_true_np, y_probs_np),\n",
    "    }\n",
    "    \n",
    "    # Only calculate AUC and MCC if we have multiple classes\n",
    "    if has_multiple_classes:\n",
    "        # Suppress the UndefinedMetricWarning\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "            metrics['AUC'] = roc_auc_score(y_true_np, y_probs_np)\n",
    "            metrics['MCC'] = matthews_corrcoef(y_true_np, y_preds)\n",
    "    else:\n",
    "        print(\"\\n⚠️ Warning: Only one class is present in the dataset.\")\n",
    "        print(\"AUC and MCC are not defined in this case and will be reported as N/A.\")\n",
    "        metrics['AUC'] = \"N/A\"\n",
    "        metrics['MCC'] = \"N/A\"\n",
    "    \n",
    "    # Convert N/A values to NaN for Excel export\n",
    "    metrics_for_excel = {k: np.nan if v == \"N/A\" else v for k, v in metrics.items()}\n",
    "    \n",
    "    # Save metrics\n",
    "    pd.DataFrame([metrics_for_excel]).to_excel(os.path.join(output_dir, \"external_metrics_cancer.xlsx\"), index=False)\n",
    "    \n",
    "    print(\"\\n🧪 External Dataset Metrics:\")\n",
    "    for key, value in metrics.items():\n",
    "        if isinstance(value, str):\n",
    "            print(f\"{key}: {value}\")\n",
    "        else:\n",
    "            print(f\"{key}: {value:.4f}\")\n",
    "    \n",
    "    # Only generate ROC curve if we have multiple classes\n",
    "    if has_multiple_classes:\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "            fpr, tpr, _ = roc_curve(y_true_np, y_probs_np)\n",
    "            \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.plot(fpr, tpr, label=f\"AUC = {metrics['AUC']:.2f}\" if isinstance(metrics['AUC'], float) else \"AUC = N/A\")\n",
    "        plt.plot([0, 1], [0, 1], '--', color='gray')\n",
    "        plt.xlabel(\"False Positive Rate\")\n",
    "        plt.ylabel(\"True Positive Rate\")\n",
    "        plt.title(\"External Dataset ROC Curve\")\n",
    "        plt.legend()\n",
    "        plt.savefig(os.path.join(output_dir, \"external_roc_cancer.png\"))\n",
    "        plt.close()\n",
    "    \n",
    "    # Create confusion matrix\n",
    "    cm = confusion_matrix(y_true_np, y_preds)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.title('Confusion Matrix - External Dataset')\n",
    "    plt.savefig(os.path.join(output_dir, \"external_confusion_matrix_cancer.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # Class distribution information\n",
    "    class_counts = np.bincount(y_true_np.astype(int))\n",
    "    print(\"\\n📊 Class Distribution in External Dataset:\")\n",
    "    for i, count in enumerate(class_counts):\n",
    "        print(f\"Class {i}: {count} samples ({count/len(y_true_np)*100:.1f}%)\")\n",
    "\n",
    "# Save all predictions\n",
    "results_df.to_excel(os.path.join(output_dir, \"external_predictions_cancer.xlsx\"), index=False)\n",
    "print(f\"\\n💾 Predictions for {len(results_df)} molecules saved to {os.path.join(output_dir, 'external_predictions_cancer.xlsx')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7c16fb-6bd2-4cde-a87d-bc2c7a508aeb",
   "metadata": {},
   "source": [
    "# Explinibilty using Integrated Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968450ac-2fc5-4b1f-8adf-6647bbb4328c",
   "metadata": {},
   "source": [
    "##  Dataset used dataset_main.xlsx\n",
    "- Top 40 most contributing SMILES plot\n",
    "- Feature Importance plot and saved the values in xlsx\n",
    "- Node importance vlaue saved in xlsx\n",
    "- Atom Type Importance plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b904832d-364c-4321-91e5-840ac4f35bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.colors import Normalize\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw, AllChem\n",
    "from rdkit.Chem.Draw import rdMolDraw2D\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.nn import GATConv, global_mean_pool  # Added import for global_mean_pool\n",
    "import seaborn as sns\n",
    "from captum.attr import IntegratedGradients\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "import json\n",
    "\n",
    "# Create output directory\n",
    "output_dir = os.path.join('GAT_result', 'external_gat_cancer')\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Define the GAT model class (needed for loading the model)\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers=3, heads=4, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        \n",
    "        # First layer has in_channels -> hidden_channels*heads\n",
    "        self.convs.append(GATConv(in_channels, hidden_channels, heads=heads, dropout=dropout))\n",
    "        \n",
    "        # Middle layers (hidden -> hidden)\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(GATConv(hidden_channels * heads, hidden_channels, heads=heads, dropout=dropout))\n",
    "        \n",
    "        # Last layer (for prediction)\n",
    "        self.convs.append(GATConv(hidden_channels * heads, out_channels, heads=1, concat=False, dropout=dropout))\n",
    "        \n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, batch):\n",
    "        for conv in self.convs[:-1]:\n",
    "            x = conv(x, edge_index)\n",
    "            x = torch.nn.functional.relu(x)\n",
    "            x = torch.nn.functional.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.convs[-1](x, edge_index)\n",
    "        return global_mean_pool(x, batch)  # Fixed: Using imported global_mean_pool\n",
    "\n",
    "# Function to convert molecules to graph data (from original code)\n",
    "def atom_features(atom):\n",
    "    return torch.tensor([\n",
    "        # Basic properties\n",
    "        atom.GetAtomicNum(),                     # Atomic number\n",
    "        atom.GetDegree(),                        # Number of bonded neighbors\n",
    "        atom.GetFormalCharge(),                  # Formal charge\n",
    "        atom.GetNumRadicalElectrons(),           # Number of radical electrons\n",
    "        int(atom.GetIsAromatic()),               # Aromaticity flag\n",
    "        \n",
    "        # Extended properties\n",
    "        atom.GetExplicitValence(),               # Explicit valence\n",
    "        atom.GetImplicitValence(),               # Implicit valence\n",
    "        atom.GetTotalValence(),                  # Total valence\n",
    "        atom.GetNumImplicitHs(),                 # Number of implicit hydrogens\n",
    "        atom.GetHybridization(),                 # Hybridization state\n",
    "        atom.GetTotalNumHs(),                    # Total number of hydrogens\n",
    "        \n",
    "        # Topological properties\n",
    "        int(atom.IsInRing()),                    # Whether the atom is in a ring\n",
    "        int(atom.IsInRingSize(3)),               # Whether in 3-membered ring\n",
    "        int(atom.IsInRingSize(4)),               # Whether in 4-membered ring\n",
    "        int(atom.IsInRingSize(5)),               # Whether in 5-membered ring\n",
    "        int(atom.IsInRingSize(6)),               # Whether in 6-membered ring\n",
    "        int(atom.IsInRingSize(7)),               # Whether in 7-membered ring\n",
    "        \n",
    "        # Electronic properties\n",
    "        atom.GetChiralTag(),                     # Chirality\n",
    "        atom.GetMass(),                          # Atomic mass\n",
    "        Chem.rdMolDescriptors.CalcCrippenDescriptors(\n",
    "            Chem.MolFromSmiles(f\"[{atom.GetSymbol()}]\"))[0]  # LogP contribution\n",
    "    ], dtype=torch.float)\n",
    "\n",
    "def mol_to_graph(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None: return None\n",
    "    atoms = [atom_features(atom) for atom in mol.GetAtoms()]\n",
    "    if not atoms: return None\n",
    "    x = torch.stack(atoms, dim=0)\n",
    "    edge_index = []\n",
    "    for bond in mol.GetBonds():\n",
    "        i, j = bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()\n",
    "        edge_index += [[i, j], [j, i]]\n",
    "    edge_index = torch.tensor(edge_index).t().contiguous()\n",
    "    edge_attr = [[bond.GetBondTypeAsDouble(), bond.GetIsConjugated(), bond.IsInRing()] * 2 for bond in mol.GetBonds()]\n",
    "    edge_attr = torch.tensor(edge_attr, dtype=torch.float).reshape(-1, 3)\n",
    "    from torch_geometric.data import Data\n",
    "    return Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "\n",
    "def load_dataset(path, smiles_col='Smiles', target_col='Target'):\n",
    "    df = pd.read_excel(path)\n",
    "    graphs = []\n",
    "    for _, row in df.iterrows():\n",
    "        if pd.isna(row[smiles_col]) or pd.isna(row[target_col]):\n",
    "            continue\n",
    "        g = mol_to_graph(str(row[smiles_col]))\n",
    "        if g is not None:\n",
    "            g.y = torch.tensor([row[target_col]], dtype=torch.float)\n",
    "            g.smiles = row[smiles_col]\n",
    "            graphs.append(g)\n",
    "    return graphs\n",
    "\n",
    "# Load the trained model (updated for GAT)\n",
    "def load_trained_model(model_path, input_dim=20, hidden_params_path=os.path.join('GAT_result', 'study_best_params_gat.json')):\n",
    "    try:\n",
    "        with open(hidden_params_path, 'r') as f:\n",
    "            best_params = json.load(f)\n",
    "        \n",
    "        model = GAT(\n",
    "            in_channels=input_dim,\n",
    "            hidden_channels=best_params['hidden_channels'],\n",
    "            out_channels=1,\n",
    "            num_layers=best_params['num_layers'],\n",
    "            heads=best_params['heads'],\n",
    "            dropout=best_params['dropout']\n",
    "        )\n",
    "    except FileNotFoundError:\n",
    "        # Fallback to default parameters if file not found\n",
    "        print(f\"Parameters file not found at {hidden_params_path}. Using default parameters.\")\n",
    "        model = GAT(\n",
    "            in_channels=input_dim,\n",
    "            hidden_channels=64,\n",
    "            out_channels=1,\n",
    "            num_layers=3,\n",
    "            heads=4,\n",
    "            dropout=0.3\n",
    "        )\n",
    "    \n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# Load model (updated path for GAT)\n",
    "model_path = os.path.join('GAT_result', 'gat_best_model.pth')\n",
    "model = load_trained_model(model_path)\n",
    "\n",
    "# Load dataset (reusing the function from your original code)\n",
    "train_data = load_dataset(\"cancer_set.xlsx\")  # Load your full dataset\n",
    "\n",
    "# Helper function to create a wrapper model for attribution methods\n",
    "class GATWrapper(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        \n",
    "    def forward(self, x, edge_index, edge_attr, batch):\n",
    "        return torch.sigmoid(self.model(x, edge_index, edge_attr, batch))\n",
    "\n",
    "# Initialize attribution method\n",
    "wrapper_model = GATWrapper(model)\n",
    "integrated_gradients = IntegratedGradients(wrapper_model)\n",
    "\n",
    "# Function to interpret molecule graph and get node/edge importance\n",
    "def interpret_molecule(graph):\n",
    "    graph_batch = graph.clone()\n",
    "    graph_batch.batch = torch.zeros(graph.x.shape[0], dtype=torch.long)\n",
    "    \n",
    "    # Get integrated gradients attribution\n",
    "    node_attr = integrated_gradients.attribute(\n",
    "        graph.x, \n",
    "        additional_forward_args=(graph.edge_index, graph.edge_attr, graph_batch.batch),\n",
    "        internal_batch_size=1\n",
    "    )\n",
    "    \n",
    "    # Sum across feature dimensions to get per-node importance\n",
    "    node_importance = torch.abs(node_attr).sum(dim=1).detach().numpy()\n",
    "    \n",
    "    # For edge importance, we'll use a simple approximation based on connected nodes\n",
    "    edge_importance = []\n",
    "    for i in range(graph.edge_index.shape[1]):\n",
    "        src, dst = graph.edge_index[0, i].item(), graph.edge_index[1, i].item()\n",
    "        edge_imp = (node_importance[src] + node_importance[dst]) / 2\n",
    "        edge_importance.append(edge_imp)\n",
    "    \n",
    "    return node_importance, np.array(edge_importance)\n",
    "\n",
    "# Function to create a safe filename from SMILES\n",
    "def create_safe_filename(smiles):\n",
    "    filename = f\"{smiles}_gat.png\"\n",
    "    # Replace characters that are problematic in filenames\n",
    "    for char in ['/', '\\\\', ':', '*', '?', '\"', '<', '>', '|']:\n",
    "        filename = filename.replace(char, '_')\n",
    "    # Ensure the filename isn't too long for the filesystem\n",
    "    if len(filename) > 200:\n",
    "        # Truncate but keep the _gat.png suffix\n",
    "        filename = filename[:195] + \"_gat_cancer.png\"\n",
    "    return filename\n",
    "\n",
    "# Function to visualize molecule with importance highlighting\n",
    "def visualize_molecule_importance(smiles, node_importance, mol=None, title=None, size=(600, 600)):\n",
    "    if mol is None:\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "    \n",
    "    # Normalize importances for coloring\n",
    "    if len(node_importance) > 0:\n",
    "        norm = Normalize(vmin=np.min(node_importance), vmax=np.max(node_importance))\n",
    "        atom_colors = {}\n",
    "        for i, imp in enumerate(node_importance):\n",
    "            color_val = norm(imp)\n",
    "            atom_colors[i] = tuple(plt.cm.viridis(color_val)[:3])\n",
    "    else:\n",
    "        atom_colors = {}\n",
    "    \n",
    "    # Create drawing\n",
    "    drawer = rdMolDraw2D.MolDraw2DCairo(size[0], size[1])\n",
    "    drawer.SetFontSize(0.8)\n",
    "    \n",
    "    # Set up atom highlights\n",
    "    highlight_atoms = list(range(mol.GetNumAtoms()))\n",
    "    highlight_bonds = []\n",
    "    highlight_atom_colors = atom_colors\n",
    "    highlight_bond_colors = {}\n",
    "    \n",
    "    # Add title if provided\n",
    "    if title:\n",
    "        drawer.DrawMoleculeWithHighlights(\n",
    "            mol, title, \n",
    "            highlightAtoms=highlight_atoms,\n",
    "            highlightAtomColors=highlight_atom_colors\n",
    "        )\n",
    "    else:\n",
    "        # Draw the molecule with highlights\n",
    "        drawer.DrawMolecule(\n",
    "            mol,\n",
    "            highlightAtoms=highlight_atoms,\n",
    "            highlightAtomColors=highlight_atom_colors\n",
    "        )\n",
    "    drawer.FinishDrawing()\n",
    "    \n",
    "    return drawer.GetDrawingText()\n",
    "\n",
    "# Function to analyze feature importance across the dataset\n",
    "def analyze_feature_importance(data_list, top_n=20):\n",
    "    all_attributions = []\n",
    "    \n",
    "    for graph in tqdm(data_list, desc=\"Analyzing feature importance\"):\n",
    "        graph_batch = graph.clone()\n",
    "        graph_batch.batch = torch.zeros(graph.x.shape[0], dtype=torch.long)\n",
    "        \n",
    "        attr = integrated_gradients.attribute(\n",
    "            graph.x, \n",
    "            additional_forward_args=(graph.edge_index, graph.edge_attr, graph_batch.batch),\n",
    "            internal_batch_size=1\n",
    "        )\n",
    "        \n",
    "        # Average attribution per feature across all nodes\n",
    "        avg_attr = torch.abs(attr).mean(dim=0).detach().numpy()\n",
    "        all_attributions.append(avg_attr)\n",
    "    \n",
    "    # Average across all molecules\n",
    "    feature_importance = np.mean(all_attributions, axis=0)\n",
    "    \n",
    "    # Feature names (same as in atom_features function)\n",
    "    feature_names = [\n",
    "        'Atomic number', 'Degree', 'Formal charge', 'Radical electrons', 'Is aromatic',\n",
    "        'Explicit valence', 'Implicit valence', 'Total valence', 'Implicit Hs',\n",
    "        'Hybridization', 'Total Hs', 'In ring', 'In ring size 3', 'In ring size 4',\n",
    "        'In ring size 5', 'In ring size 6', 'In ring size 7', 'Chiral tag', 'Mass',\n",
    "        'LogP contribution'\n",
    "    ]\n",
    "    \n",
    "    # Sort by importance\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': feature_importance\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    return importance_df\n",
    "\n",
    "# Create output subfolders\n",
    "viz_dir = os.path.join(output_dir, 'molecule_viz_cancer')\n",
    "os.makedirs(viz_dir, exist_ok=True)\n",
    "\n",
    "top_molecules_dir = os.path.join(output_dir, 'top_molecules')\n",
    "os.makedirs(top_molecules_dir, exist_ok=True)\n",
    "\n",
    "# Process all molecules in the dataset\n",
    "molecule_importances = []\n",
    "\n",
    "# Process a subset of molecules (limit to 100 for efficiency)\n",
    "num_molecules = min(100, len(train_data))\n",
    "\n",
    "for i, graph in enumerate(tqdm(train_data[:num_molecules], desc=\"Processing molecules\")):\n",
    "    try:\n",
    "        smiles = graph.smiles\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is None:\n",
    "            continue\n",
    "            \n",
    "        # Get prediction score\n",
    "        graph_batch = graph.clone()\n",
    "        graph_batch.batch = torch.zeros(graph.x.shape[0], dtype=torch.long)\n",
    "        pred_score = wrapper_model(graph.x, graph.edge_index, graph.edge_attr, graph_batch.batch).item()\n",
    "        \n",
    "        # Get node and edge importance\n",
    "        node_importance, edge_importance = interpret_molecule(graph)\n",
    "        \n",
    "        # Calculate importance metrics\n",
    "        avg_importance = np.mean(node_importance)\n",
    "        max_importance = np.max(node_importance)\n",
    "        \n",
    "        # Create a safe filename from the SMILES\n",
    "        safe_filename = create_safe_filename(smiles)\n",
    "        \n",
    "        # Save molecule image with importance visualization\n",
    "        img_data = visualize_molecule_importance(smiles, node_importance, mol)\n",
    "        with open(os.path.join(viz_dir, safe_filename), \"wb\") as f:\n",
    "            f.write(img_data)\n",
    "        \n",
    "        # Add to our results dataframe\n",
    "        molecule_importances.append({\n",
    "            'Index': i+1,\n",
    "            'SMILES': smiles,\n",
    "            'Prediction': pred_score,\n",
    "            'True_Label': graph.y.item(),\n",
    "            'Average_Importance': avg_importance,\n",
    "            'Max_Importance': max_importance,\n",
    "            'Most_Important_Atom_Index': np.argmax(node_importance),\n",
    "            'Filename': safe_filename\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing molecule {i+1}: {e}\")\n",
    "\n",
    "# Create and save dataframe of molecule importances\n",
    "importance_df = pd.DataFrame(molecule_importances)\n",
    "importance_df.to_excel(os.path.join(output_dir, 'molecule_importances_cancer.xlsx'), index=False)\n",
    "\n",
    "# Create feature importance analysis\n",
    "feature_imp_df = analyze_feature_importance(train_data[:num_molecules])\n",
    "feature_imp_df.to_excel(os.path.join(output_dir, 'feature_importance_cancer.xlsx'), index=False)\n",
    "\n",
    "# Plot and save feature importance graph\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Importance', y='Feature', data=feature_imp_df.head(15))\n",
    "plt.title('Top 15 Important Atom Features (GAT Analysis)')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, 'feature_importance_cancer.png'), dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# Find the top 40 molecules by importance\n",
    "top_molecules_count = min(40, len(importance_df))\n",
    "top_mols = importance_df.sort_values('Average_Importance', ascending=False).head(top_molecules_count)\n",
    "\n",
    "# Create a top SMILES CSV file\n",
    "with open(os.path.join(output_dir, \"top_smiles_cancer.csv\"), \"w\") as f:\n",
    "    f.write(\"Rank,MoleculeIndex,SMILES,Prediction,TrueLabel,AverageImportance,Filename\\n\")\n",
    "    for j, (_, r) in enumerate(top_mols.iterrows()):\n",
    "        f.write(f\"{j+1},{r['Index']},{r['SMILES']},{r['Prediction']:.4f},{r['True_Label']:.0f},{r['Average_Importance']:.4f},{r['Filename']}\\n\")\n",
    "\n",
    "# Create a higher-quality version of the top molecules (with titles)\n",
    "for i, (_, row) in enumerate(tqdm(top_mols.iterrows(), desc=\"Creating high-quality top molecule images\")):\n",
    "    try:\n",
    "        # Get the molecule and its data\n",
    "        idx = row['Index']\n",
    "        smiles = row['SMILES']\n",
    "        pred = row['Prediction']\n",
    "        true_label = row['True_Label']\n",
    "        \n",
    "        # Get the corresponding graph data\n",
    "        graph = train_data[idx-1]  # -1 because indices start at 1 in our data\n",
    "        \n",
    "        # Re-interpret the molecule for visualization\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is None:\n",
    "            continue\n",
    "            \n",
    "        # Get node importance\n",
    "        node_importance, _ = interpret_molecule(graph)\n",
    "        \n",
    "        # Create a title with info\n",
    "        title = f\"#{idx}: Pred={pred:.3f}, True={true_label:.0f}, Importance={row['Average_Importance']:.3f}\"\n",
    "        \n",
    "        # Create a high-quality visualization\n",
    "        img_data = visualize_molecule_importance(smiles, node_importance, mol, title=title, size=(800, 800))\n",
    "        \n",
    "        # Use the same filename as before\n",
    "        safe_filename = row['Filename']\n",
    "        \n",
    "        # Save to the top molecules directory\n",
    "        with open(os.path.join(top_molecules_dir, safe_filename), \"wb\") as f:\n",
    "            f.write(img_data)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating top molecule image for rank {i+1}: {e}\")\n",
    "\n",
    "# Create node importance analysis\n",
    "node_importances = []\n",
    "for i, graph in enumerate(tqdm(train_data[:num_molecules], desc=\"Analyzing node types\")):\n",
    "    try:\n",
    "        smiles = graph.smiles\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is None:\n",
    "            continue\n",
    "            \n",
    "        node_importance, _ = interpret_molecule(graph)\n",
    "        \n",
    "        # Get atom types and their importance\n",
    "        for atom_idx, importance in enumerate(node_importance):\n",
    "            atom_symbol = mol.GetAtomWithIdx(atom_idx).GetSymbol()\n",
    "            node_importances.append({\n",
    "                'Molecule_Index': i+1,\n",
    "                'SMILES': smiles,\n",
    "                'Atom_Index': atom_idx,\n",
    "                'Atom_Symbol': atom_symbol,\n",
    "                'Importance': importance\n",
    "            })\n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing node for molecule {i+1}: {e}\")\n",
    "\n",
    "# Create and save node importance dataframe\n",
    "node_imp_df = pd.DataFrame(node_importances)\n",
    "node_imp_df.to_excel(os.path.join(output_dir, 'node_importances_cancer.xlsx'), index=False)\n",
    "\n",
    "# Analyze importance by atom type\n",
    "atom_importance = node_imp_df.groupby('Atom_Symbol')['Importance'].mean().reset_index()\n",
    "atom_importance = atom_importance.sort_values('Importance', ascending=False)\n",
    "atom_importance.to_excel(os.path.join(output_dir, 'atom_type_importance_cancer.xlsx'), index=False)\n",
    "\n",
    "# Plot atom type importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Importance', y='Atom_Symbol', data=atom_importance)\n",
    "plt.title('Average Importance by Atom Type (GAT Analysis)')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, 'atom_type_importance_cancer.png'), dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# Create a color-coded Excel file with molecule importances\n",
    "try:\n",
    "    from openpyxl import load_workbook\n",
    "    from openpyxl.styles import PatternFill\n",
    "    from openpyxl.styles.differential import DifferentialStyle\n",
    "    from openpyxl.formatting.rule import ColorScaleRule\n",
    "    \n",
    "    color_excel_path = os.path.join(output_dir, 'molecule_importances_colored_cancer.xlsx')\n",
    "    importance_df.to_excel(color_excel_path, index=False)\n",
    "    \n",
    "    # Open the workbook and add color scales\n",
    "    wb = load_workbook(color_excel_path)\n",
    "    ws = wb.active\n",
    "    \n",
    "    # Add color scale to Average_Importance column\n",
    "    col_idx = importance_df.columns.get_loc(\"Average_Importance\") + 1  # +1 because Excel is 1-indexed\n",
    "    col_letter = ws.cell(1, col_idx).column_letter\n",
    "    \n",
    "    ws.conditional_formatting.add(\n",
    "        f\"{col_letter}2:{col_letter}{len(importance_df)+1}\",\n",
    "        ColorScaleRule(\n",
    "            start_type='min', start_color='FFFFFF',\n",
    "            mid_type='percentile', mid_value=50, mid_color='FFFF00',\n",
    "            end_type='max', end_color='FF0000'\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Add color scale to Max_Importance column\n",
    "    col_idx = importance_df.columns.get_loc(\"Max_Importance\") + 1\n",
    "    col_letter = ws.cell(1, col_idx).column_letter\n",
    "    \n",
    "    ws.conditional_formatting.add(\n",
    "        f\"{col_letter}2:{col_letter}{len(importance_df)+1}\",\n",
    "        ColorScaleRule(\n",
    "            start_type='min', start_color='FFFFFF',\n",
    "            mid_type='percentile', mid_value=50, mid_color='FFFF00',\n",
    "            end_type='max', end_color='FF0000'\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Save the workbook\n",
    "    wb.save(color_excel_path)\n",
    "    print(f\"Created color-coded Excel file: {color_excel_path}\")\n",
    "except ImportError:\n",
    "    print(\"openpyxl not available - skipping colored Excel creation\")\n",
    "\n",
    "print(f\"✅ Analysis complete! All results saved to {output_dir}\")\n",
    "print(f\"📊 Key files generated:\")\n",
    "print(f\" - Molecule visualizations with SMILES filenames: {viz_dir}/*.png\")\n",
    "print(f\" - Top 40 molecules: {top_molecules_dir}/*.png\")\n",
    "print(f\" - Top molecules SMILES list: {os.path.join(output_dir, 'top_smiles_cancer.csv')}\")\n",
    "print(f\" - Feature importance analysis: {os.path.join(output_dir, 'feature_importance_cancer.xlsx')}\")\n",
    "print(f\" - Molecule importance spreadsheet: {os.path.join(output_dir, 'molecule_importances_cancer.xlsx')}\")\n",
    "print(f\" - Atom type importance: {os.path.join(output_dir, 'atom_type_importance_cancer.png')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77eb130-a55a-4cb9-8c85-fb3208c0c2f6",
   "metadata": {},
   "source": [
    "##  Dataset used cancer_set.xlsx(FDA approved)\n",
    "- Top 40 most contributing SMILES plot\n",
    "- Feature Importance plot and saved the values in xlsx\n",
    "- Node importance vlaue saved in xlsx\n",
    "- Atom Type Importance plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e1ba7e-f22b-4f07-bf07-1c9e95d9a355",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.colors import Normalize\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw, AllChem\n",
    "from rdkit.Chem.Draw import rdMolDraw2D\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.nn import GATConv, global_mean_pool  # Added import for global_mean_pool\n",
    "import seaborn as sns\n",
    "from captum.attr import IntegratedGradients\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "import json\n",
    "\n",
    "# Create output directory\n",
    "output_dir = os.path.join('GAT_result', 'external_gat')\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Define the GAT model class (needed for loading the model)\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers=3, heads=4, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        \n",
    "        # First layer has in_channels -> hidden_channels*heads\n",
    "        self.convs.append(GATConv(in_channels, hidden_channels, heads=heads, dropout=dropout))\n",
    "        \n",
    "        # Middle layers (hidden -> hidden)\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(GATConv(hidden_channels * heads, hidden_channels, heads=heads, dropout=dropout))\n",
    "        \n",
    "        # Last layer (for prediction)\n",
    "        self.convs.append(GATConv(hidden_channels * heads, out_channels, heads=1, concat=False, dropout=dropout))\n",
    "        \n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, batch):\n",
    "        for conv in self.convs[:-1]:\n",
    "            x = conv(x, edge_index)\n",
    "            x = torch.nn.functional.relu(x)\n",
    "            x = torch.nn.functional.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.convs[-1](x, edge_index)\n",
    "        return global_mean_pool(x, batch)  # Fixed: Using imported global_mean_pool\n",
    "\n",
    "# Function to convert molecules to graph data (from original code)\n",
    "def atom_features(atom):\n",
    "    return torch.tensor([\n",
    "        # Basic properties\n",
    "        atom.GetAtomicNum(),                     # Atomic number\n",
    "        atom.GetDegree(),                        # Number of bonded neighbors\n",
    "        atom.GetFormalCharge(),                  # Formal charge\n",
    "        atom.GetNumRadicalElectrons(),           # Number of radical electrons\n",
    "        int(atom.GetIsAromatic()),               # Aromaticity flag\n",
    "        \n",
    "        # Extended properties\n",
    "        atom.GetExplicitValence(),               # Explicit valence\n",
    "        atom.GetImplicitValence(),               # Implicit valence\n",
    "        atom.GetTotalValence(),                  # Total valence\n",
    "        atom.GetNumImplicitHs(),                 # Number of implicit hydrogens\n",
    "        atom.GetHybridization(),                 # Hybridization state\n",
    "        atom.GetTotalNumHs(),                    # Total number of hydrogens\n",
    "        \n",
    "        # Topological properties\n",
    "        int(atom.IsInRing()),                    # Whether the atom is in a ring\n",
    "        int(atom.IsInRingSize(3)),               # Whether in 3-membered ring\n",
    "        int(atom.IsInRingSize(4)),               # Whether in 4-membered ring\n",
    "        int(atom.IsInRingSize(5)),               # Whether in 5-membered ring\n",
    "        int(atom.IsInRingSize(6)),               # Whether in 6-membered ring\n",
    "        int(atom.IsInRingSize(7)),               # Whether in 7-membered ring\n",
    "        \n",
    "        # Electronic properties\n",
    "        atom.GetChiralTag(),                     # Chirality\n",
    "        atom.GetMass(),                          # Atomic mass\n",
    "        Chem.rdMolDescriptors.CalcCrippenDescriptors(\n",
    "            Chem.MolFromSmiles(f\"[{atom.GetSymbol()}]\"))[0]  # LogP contribution\n",
    "    ], dtype=torch.float)\n",
    "\n",
    "def mol_to_graph(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None: return None\n",
    "    atoms = [atom_features(atom) for atom in mol.GetAtoms()]\n",
    "    if not atoms: return None\n",
    "    x = torch.stack(atoms, dim=0)\n",
    "    edge_index = []\n",
    "    for bond in mol.GetBonds():\n",
    "        i, j = bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()\n",
    "        edge_index += [[i, j], [j, i]]\n",
    "    edge_index = torch.tensor(edge_index).t().contiguous()\n",
    "    edge_attr = [[bond.GetBondTypeAsDouble(), bond.GetIsConjugated(), bond.IsInRing()] * 2 for bond in mol.GetBonds()]\n",
    "    edge_attr = torch.tensor(edge_attr, dtype=torch.float).reshape(-1, 3)\n",
    "    from torch_geometric.data import Data\n",
    "    return Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "\n",
    "def load_dataset(path, smiles_col='Smiles', target_col='Target'):\n",
    "    df = pd.read_excel(path)\n",
    "    graphs = []\n",
    "    for _, row in df.iterrows():\n",
    "        if pd.isna(row[smiles_col]) or pd.isna(row[target_col]):\n",
    "            continue\n",
    "        g = mol_to_graph(str(row[smiles_col]))\n",
    "        if g is not None:\n",
    "            g.y = torch.tensor([row[target_col]], dtype=torch.float)\n",
    "            g.smiles = row[smiles_col]\n",
    "            graphs.append(g)\n",
    "    return graphs\n",
    "\n",
    "# Load the trained model (updated for GAT)\n",
    "def load_trained_model(model_path, input_dim=20, hidden_params_path=os.path.join('GAT_result', 'study_best_params_gat.json')):\n",
    "    try:\n",
    "        with open(hidden_params_path, 'r') as f:\n",
    "            best_params = json.load(f)\n",
    "        \n",
    "        model = GAT(\n",
    "            in_channels=input_dim,\n",
    "            hidden_channels=best_params['hidden_channels'],\n",
    "            out_channels=1,\n",
    "            num_layers=best_params['num_layers'],\n",
    "            heads=best_params['heads'],\n",
    "            dropout=best_params['dropout']\n",
    "        )\n",
    "    except FileNotFoundError:\n",
    "        # Fallback to default parameters if file not found\n",
    "        print(f\"Parameters file not found at {hidden_params_path}. Using default parameters.\")\n",
    "        model = GAT(\n",
    "            in_channels=input_dim,\n",
    "            hidden_channels=64,\n",
    "            out_channels=1,\n",
    "            num_layers=3,\n",
    "            heads=4,\n",
    "            dropout=0.3\n",
    "        )\n",
    "    \n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# Load model (updated path for GAT)\n",
    "model_path = os.path.join('GAT_result', 'gat_best_model.pth')\n",
    "model = load_trained_model(model_path)\n",
    "\n",
    "# Load dataset (reusing the function from your original code)\n",
    "train_data = load_dataset(\"dataset_main.xlsx\")  # Load your full dataset\n",
    "\n",
    "# Helper function to create a wrapper model for attribution methods\n",
    "class GATWrapper(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        \n",
    "    def forward(self, x, edge_index, edge_attr, batch):\n",
    "        return torch.sigmoid(self.model(x, edge_index, edge_attr, batch))\n",
    "\n",
    "# Initialize attribution method\n",
    "wrapper_model = GATWrapper(model)\n",
    "integrated_gradients = IntegratedGradients(wrapper_model)\n",
    "\n",
    "# Function to interpret molecule graph and get node/edge importance\n",
    "def interpret_molecule(graph):\n",
    "    graph_batch = graph.clone()\n",
    "    graph_batch.batch = torch.zeros(graph.x.shape[0], dtype=torch.long)\n",
    "    \n",
    "    # Get integrated gradients attribution\n",
    "    node_attr = integrated_gradients.attribute(\n",
    "        graph.x, \n",
    "        additional_forward_args=(graph.edge_index, graph.edge_attr, graph_batch.batch),\n",
    "        internal_batch_size=1\n",
    "    )\n",
    "    \n",
    "    # Sum across feature dimensions to get per-node importance\n",
    "    node_importance = torch.abs(node_attr).sum(dim=1).detach().numpy()\n",
    "    \n",
    "    # For edge importance, we'll use a simple approximation based on connected nodes\n",
    "    edge_importance = []\n",
    "    for i in range(graph.edge_index.shape[1]):\n",
    "        src, dst = graph.edge_index[0, i].item(), graph.edge_index[1, i].item()\n",
    "        edge_imp = (node_importance[src] + node_importance[dst]) / 2\n",
    "        edge_importance.append(edge_imp)\n",
    "    \n",
    "    return node_importance, np.array(edge_importance)\n",
    "\n",
    "# Function to create a safe filename from SMILES\n",
    "def create_safe_filename(smiles):\n",
    "    filename = f\"{smiles}_gat.png\"\n",
    "    # Replace characters that are problematic in filenames\n",
    "    for char in ['/', '\\\\', ':', '*', '?', '\"', '<', '>', '|']:\n",
    "        filename = filename.replace(char, '_')\n",
    "    # Ensure the filename isn't too long for the filesystem\n",
    "    if len(filename) > 200:\n",
    "        # Truncate but keep the _gat.png suffix\n",
    "        filename = filename[:195] + \"_gat.png\"\n",
    "    return filename\n",
    "\n",
    "# Function to visualize molecule with importance highlighting\n",
    "def visualize_molecule_importance(smiles, node_importance, mol=None, title=None, size=(600, 600)):\n",
    "    if mol is None:\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "    \n",
    "    # Normalize importances for coloring\n",
    "    if len(node_importance) > 0:\n",
    "        norm = Normalize(vmin=np.min(node_importance), vmax=np.max(node_importance))\n",
    "        atom_colors = {}\n",
    "        for i, imp in enumerate(node_importance):\n",
    "            color_val = norm(imp)\n",
    "            atom_colors[i] = tuple(plt.cm.viridis(color_val)[:3])\n",
    "    else:\n",
    "        atom_colors = {}\n",
    "    \n",
    "    # Create drawing\n",
    "    drawer = rdMolDraw2D.MolDraw2DCairo(size[0], size[1])\n",
    "    drawer.SetFontSize(0.8)\n",
    "    \n",
    "    # Set up atom highlights\n",
    "    highlight_atoms = list(range(mol.GetNumAtoms()))\n",
    "    highlight_bonds = []\n",
    "    highlight_atom_colors = atom_colors\n",
    "    highlight_bond_colors = {}\n",
    "    \n",
    "    # Add title if provided\n",
    "    if title:\n",
    "        drawer.DrawMoleculeWithHighlights(\n",
    "            mol, title, \n",
    "            highlightAtoms=highlight_atoms,\n",
    "            highlightAtomColors=highlight_atom_colors\n",
    "        )\n",
    "    else:\n",
    "        # Draw the molecule with highlights\n",
    "        drawer.DrawMolecule(\n",
    "            mol,\n",
    "            highlightAtoms=highlight_atoms,\n",
    "            highlightAtomColors=highlight_atom_colors\n",
    "        )\n",
    "    drawer.FinishDrawing()\n",
    "    \n",
    "    return drawer.GetDrawingText()\n",
    "\n",
    "# Function to analyze feature importance across the dataset\n",
    "def analyze_feature_importance(data_list, top_n=20):\n",
    "    all_attributions = []\n",
    "    \n",
    "    for graph in tqdm(data_list, desc=\"Analyzing feature importance\"):\n",
    "        graph_batch = graph.clone()\n",
    "        graph_batch.batch = torch.zeros(graph.x.shape[0], dtype=torch.long)\n",
    "        \n",
    "        attr = integrated_gradients.attribute(\n",
    "            graph.x, \n",
    "            additional_forward_args=(graph.edge_index, graph.edge_attr, graph_batch.batch),\n",
    "            internal_batch_size=1\n",
    "        )\n",
    "        \n",
    "        # Average attribution per feature across all nodes\n",
    "        avg_attr = torch.abs(attr).mean(dim=0).detach().numpy()\n",
    "        all_attributions.append(avg_attr)\n",
    "    \n",
    "    # Average across all molecules\n",
    "    feature_importance = np.mean(all_attributions, axis=0)\n",
    "    \n",
    "    # Feature names (same as in atom_features function)\n",
    "    feature_names = [\n",
    "        'Atomic number', 'Degree', 'Formal charge', 'Radical electrons', 'Is aromatic',\n",
    "        'Explicit valence', 'Implicit valence', 'Total valence', 'Implicit Hs',\n",
    "        'Hybridization', 'Total Hs', 'In ring', 'In ring size 3', 'In ring size 4',\n",
    "        'In ring size 5', 'In ring size 6', 'In ring size 7', 'Chiral tag', 'Mass',\n",
    "        'LogP contribution'\n",
    "    ]\n",
    "    \n",
    "    # Sort by importance\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': feature_importance\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    return importance_df\n",
    "\n",
    "# Create output subfolders\n",
    "viz_dir = os.path.join(output_dir, 'molecule_viz_main')\n",
    "os.makedirs(viz_dir, exist_ok=True)\n",
    "\n",
    "top_molecules_dir = os.path.join(output_dir, 'top_molecules')\n",
    "os.makedirs(top_molecules_dir, exist_ok=True)\n",
    "\n",
    "# Process all molecules in the dataset\n",
    "molecule_importances = []\n",
    "\n",
    "# Process a subset of molecules (limit to 100 for efficiency)\n",
    "num_molecules = min(100, len(train_data))\n",
    "\n",
    "for i, graph in enumerate(tqdm(train_data[:num_molecules], desc=\"Processing molecules\")):\n",
    "    try:\n",
    "        smiles = graph.smiles\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is None:\n",
    "            continue\n",
    "            \n",
    "        # Get prediction score\n",
    "        graph_batch = graph.clone()\n",
    "        graph_batch.batch = torch.zeros(graph.x.shape[0], dtype=torch.long)\n",
    "        pred_score = wrapper_model(graph.x, graph.edge_index, graph.edge_attr, graph_batch.batch).item()\n",
    "        \n",
    "        # Get node and edge importance\n",
    "        node_importance, edge_importance = interpret_molecule(graph)\n",
    "        \n",
    "        # Calculate importance metrics\n",
    "        avg_importance = np.mean(node_importance)\n",
    "        max_importance = np.max(node_importance)\n",
    "        \n",
    "        # Create a safe filename from the SMILES\n",
    "        safe_filename = create_safe_filename(smiles)\n",
    "        \n",
    "        # Save molecule image with importance visualization\n",
    "        img_data = visualize_molecule_importance(smiles, node_importance, mol)\n",
    "        with open(os.path.join(viz_dir, safe_filename), \"wb\") as f:\n",
    "            f.write(img_data)\n",
    "        \n",
    "        # Add to our results dataframe\n",
    "        molecule_importances.append({\n",
    "            'Index': i+1,\n",
    "            'SMILES': smiles,\n",
    "            'Prediction': pred_score,\n",
    "            'True_Label': graph.y.item(),\n",
    "            'Average_Importance': avg_importance,\n",
    "            'Max_Importance': max_importance,\n",
    "            'Most_Important_Atom_Index': np.argmax(node_importance),\n",
    "            'Filename': safe_filename\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing molecule {i+1}: {e}\")\n",
    "\n",
    "# Create and save dataframe of molecule importances\n",
    "importance_df = pd.DataFrame(molecule_importances)\n",
    "importance_df.to_excel(os.path.join(output_dir, 'molecule_importances_main.xlsx'), index=False)\n",
    "\n",
    "# Create feature importance analysis\n",
    "feature_imp_df = analyze_feature_importance(train_data[:num_molecules])\n",
    "feature_imp_df.to_excel(os.path.join(output_dir, 'feature_importance_main.xlsx'), index=False)\n",
    "\n",
    "# Plot and save feature importance graph\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Importance', y='Feature', data=feature_imp_df.head(15))\n",
    "plt.title('Top 15 Important Atom Features (GAT Analysis)')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, 'feature_importance_main.png'), dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# Find the top 40 molecules by importance\n",
    "top_molecules_count = min(40, len(importance_df))\n",
    "top_mols = importance_df.sort_values('Average_Importance', ascending=False).head(top_molecules_count)\n",
    "\n",
    "# Create a top SMILES CSV file\n",
    "with open(os.path.join(output_dir, \"top_smiles_cancer.csv\"), \"w\") as f:\n",
    "    f.write(\"Rank,MoleculeIndex,SMILES,Prediction,TrueLabel,AverageImportance,Filename\\n\")\n",
    "    for j, (_, r) in enumerate(top_mols.iterrows()):\n",
    "        f.write(f\"{j+1},{r['Index']},{r['SMILES']},{r['Prediction']:.4f},{r['True_Label']:.0f},{r['Average_Importance']:.4f},{r['Filename']}\\n\")\n",
    "\n",
    "# Create a higher-quality version of the top molecules (with titles)\n",
    "for i, (_, row) in enumerate(tqdm(top_mols.iterrows(), desc=\"Creating high-quality top molecule images\")):\n",
    "    try:\n",
    "        # Get the molecule and its data\n",
    "        idx = row['Index']\n",
    "        smiles = row['SMILES']\n",
    "        pred = row['Prediction']\n",
    "        true_label = row['True_Label']\n",
    "        \n",
    "        # Get the corresponding graph data\n",
    "        graph = train_data[idx-1]  # -1 because indices start at 1 in our data\n",
    "        \n",
    "        # Re-interpret the molecule for visualization\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is None:\n",
    "            continue\n",
    "            \n",
    "        # Get node importance\n",
    "        node_importance, _ = interpret_molecule(graph)\n",
    "        \n",
    "        # Create a title with info\n",
    "        title = f\"#{idx}: Pred={pred:.3f}, True={true_label:.0f}, Importance={row['Average_Importance']:.3f}\"\n",
    "        \n",
    "        # Create a high-quality visualization\n",
    "        img_data = visualize_molecule_importance(smiles, node_importance, mol, title=title, size=(800, 800))\n",
    "        \n",
    "        # Use the same filename as before\n",
    "        safe_filename = row['Filename']\n",
    "        \n",
    "        # Save to the top molecules directory\n",
    "        with open(os.path.join(top_molecules_dir, safe_filename), \"wb\") as f:\n",
    "            f.write(img_data)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating top molecule image for rank {i+1}: {e}\")\n",
    "\n",
    "# Create node importance analysis\n",
    "node_importances = []\n",
    "for i, graph in enumerate(tqdm(train_data[:num_molecules], desc=\"Analyzing node types\")):\n",
    "    try:\n",
    "        smiles = graph.smiles\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is None:\n",
    "            continue\n",
    "            \n",
    "        node_importance, _ = interpret_molecule(graph)\n",
    "        \n",
    "        # Get atom types and their importance\n",
    "        for atom_idx, importance in enumerate(node_importance):\n",
    "            atom_symbol = mol.GetAtomWithIdx(atom_idx).GetSymbol()\n",
    "            node_importances.append({\n",
    "                'Molecule_Index': i+1,\n",
    "                'SMILES': smiles,\n",
    "                'Atom_Index': atom_idx,\n",
    "                'Atom_Symbol': atom_symbol,\n",
    "                'Importance': importance\n",
    "            })\n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing node for molecule {i+1}: {e}\")\n",
    "\n",
    "# Create and save node importance dataframe\n",
    "node_imp_df = pd.DataFrame(node_importances)\n",
    "node_imp_df.to_excel(os.path.join(output_dir, 'node_importances_main.xlsx'), index=False)\n",
    "\n",
    "# Analyze importance by atom type\n",
    "atom_importance = node_imp_df.groupby('Atom_Symbol')['Importance'].mean().reset_index()\n",
    "atom_importance = atom_importance.sort_values('Importance', ascending=False)\n",
    "atom_importance.to_excel(os.path.join(output_dir, 'atom_type_importance_main.xlsx'), index=False)\n",
    "\n",
    "# Plot atom type importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Importance', y='Atom_Symbol', data=atom_importance)\n",
    "plt.title('Average Importance by Atom Type (GAT Analysis)')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, 'atom_type_importance_main.png'), dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# Create a color-coded Excel file with molecule importances\n",
    "try:\n",
    "    from openpyxl import load_workbook\n",
    "    from openpyxl.styles import PatternFill\n",
    "    from openpyxl.styles.differential import DifferentialStyle\n",
    "    from openpyxl.formatting.rule import ColorScaleRule\n",
    "    \n",
    "    color_excel_path = os.path.join(output_dir, 'molecule_importances_colored_main.xlsx')\n",
    "    importance_df.to_excel(color_excel_path, index=False)\n",
    "    \n",
    "    # Open the workbook and add color scales\n",
    "    wb = load_workbook(color_excel_path)\n",
    "    ws = wb.active\n",
    "    \n",
    "    # Add color scale to Average_Importance column\n",
    "    col_idx = importance_df.columns.get_loc(\"Average_Importance\") + 1  # +1 because Excel is 1-indexed\n",
    "    col_letter = ws.cell(1, col_idx).column_letter\n",
    "    \n",
    "    ws.conditional_formatting.add(\n",
    "        f\"{col_letter}2:{col_letter}{len(importance_df)+1}\",\n",
    "        ColorScaleRule(\n",
    "            start_type='min', start_color='FFFFFF',\n",
    "            mid_type='percentile', mid_value=50, mid_color='FFFF00',\n",
    "            end_type='max', end_color='FF0000'\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Add color scale to Max_Importance column\n",
    "    col_idx = importance_df.columns.get_loc(\"Max_Importance\") + 1\n",
    "    col_letter = ws.cell(1, col_idx).column_letter\n",
    "    \n",
    "    ws.conditional_formatting.add(\n",
    "        f\"{col_letter}2:{col_letter}{len(importance_df)+1}\",\n",
    "        ColorScaleRule(\n",
    "            start_type='min', start_color='FFFFFF',\n",
    "            mid_type='percentile', mid_value=50, mid_color='FFFF00',\n",
    "            end_type='max', end_color='FF0000'\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Save the workbook\n",
    "    wb.save(color_excel_path)\n",
    "    print(f\"Created color-coded Excel file: {color_excel_path}\")\n",
    "except ImportError:\n",
    "    print(\"openpyxl not available - skipping colored Excel creation\")\n",
    "\n",
    "print(f\"✅ Analysis complete! All results saved to {output_dir}\")\n",
    "print(f\"📊 Key files generated:\")\n",
    "print(f\" - Molecule visualizations with SMILES filenames: {viz_dir}/*.png\")\n",
    "print(f\" - Top 40 molecules: {top_molecules_dir}/*.png\")\n",
    "print(f\" - Top molecules SMILES list: {os.path.join(output_dir, 'top_smiles_main.csv')}\")\n",
    "print(f\" - Feature importance analysis: {os.path.join(output_dir, 'feature_importance_main.xlsx')}\")\n",
    "print(f\" - Molecule importance spreadsheet: {os.path.join(output_dir, 'molecule_importances_main.xlsx')}\")\n",
    "print(f\" - Atom type importance: {os.path.join(output_dir, 'atom_type_importance_main.png')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a57c29-6a6e-4c06-b3b2-4b77b6caf356",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7e7c2f-2d42-4e43-9657-c0a06dc6526f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
